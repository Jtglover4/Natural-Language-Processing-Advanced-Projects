BPE_Tokenization_and_Generation_GPT2_Gemini_Prompting.ipynb - This script explores Byte Pair Encoding (BPE) tokenization, subword representations, and evaluates performance across English and non-English corpora. It also implements and compares decoding algorithms (Greedy, Temperature, Nucleus, and Typical Sampling) for GPT-2 Medium and demonstrates advanced prompting techniques (Zero-shot, Few-shot, Chain of Thought, Self-Consistency) using the GSM8K dataset and Google Gemini for mathematical reasoning tasks.

NLP_NGrams_LogReg_WordEmbeddings.ipynb - Implementation of N-Gram language models, logistic regression for Twitter sentiment analysis, and word embedding tasks with pre-trained GloVe vectors, including cosine similarity, word analogies, and PCA visualization.

RNN_and_FineTuning_IMDb_SentimentAnalysis.ipynb - This file implements an RNN from scratch for sequence prediction using the IMDb dataset and fine-tunes a pretrained DistillBERT model for sentiment analysis. It includes one-hot encoding, GloVe embeddings, multi-layered RNN classes, GPT-2 parameter counting exercises, and a complete training loop for fine-tuning on sentiment analysis with AdamW optimizer and cross-entropy loss.

autodiff_mlp_word2vec_self_attention.ipynb - This project implements the creation of an autodiff framework for computing Jacobians in MLPs, word embeddings using PPMI and Word2Vec (with negative sampling), and a Self-Attention module for generating context-aware embeddings. The final evaluation compares static Word2Vec embeddings with dynamic contextual embeddings on the WiC dataset using logistic regression for binary classification.
