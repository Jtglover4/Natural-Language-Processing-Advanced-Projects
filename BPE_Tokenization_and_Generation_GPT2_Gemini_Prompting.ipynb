{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b21c569c70e44f4797d3253c372adc62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d645581d43f4266991b2e557e508106",
              "IPY_MODEL_9acff515db1540de966679a200a6d513",
              "IPY_MODEL_d7da60183a55497fb94007b5dcfd5d71"
            ],
            "layout": "IPY_MODEL_e40bcf2027384917aa5e20f0a687909d"
          }
        },
        "4d645581d43f4266991b2e557e508106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78edb5011e164476997f813ff0327e47",
            "placeholder": "​",
            "style": "IPY_MODEL_f98b71c024e348d68898b07e5def8281",
            "value": "README.md: 100%"
          }
        },
        "9acff515db1540de966679a200a6d513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27d57fefd4c14f97bc419929ff9935d8",
            "max": 10464,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f55ace0efe2a4014a9ece20581943ef0",
            "value": 10464
          }
        },
        "d7da60183a55497fb94007b5dcfd5d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63629e816d964dceac6146a250321a46",
            "placeholder": "​",
            "style": "IPY_MODEL_ca7dd20ddbc94cc39893f642afd4dc73",
            "value": " 10.5k/10.5k [00:00&lt;00:00, 212kB/s]"
          }
        },
        "e40bcf2027384917aa5e20f0a687909d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78edb5011e164476997f813ff0327e47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f98b71c024e348d68898b07e5def8281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27d57fefd4c14f97bc419929ff9935d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f55ace0efe2a4014a9ece20581943ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63629e816d964dceac6146a250321a46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca7dd20ddbc94cc39893f642afd4dc73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f46e980c454a45fc90ea6b59cc779261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_222ebfa12b7444b1afc39d1b1be81243",
              "IPY_MODEL_89d555269dd4452081e80029c477b305",
              "IPY_MODEL_a0d446a517444e0ba5128cd2ae0ed757"
            ],
            "layout": "IPY_MODEL_e02760960d234936881bfe59d06b5e27"
          }
        },
        "222ebfa12b7444b1afc39d1b1be81243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4c8d134c9bc47f2a6718a048c7b0a6b",
            "placeholder": "​",
            "style": "IPY_MODEL_e43ff02af55f4a5b92556bf2c2cea202",
            "value": "test-00000-of-00001.parquet: 100%"
          }
        },
        "89d555269dd4452081e80029c477b305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fce2634169f14ba29d7c439d3c62a78d",
            "max": 732610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e33082839c034083a05d82bd54bc538b",
            "value": 732610
          }
        },
        "a0d446a517444e0ba5128cd2ae0ed757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cb7dbe1abf2440da9fbe175aeb5e754",
            "placeholder": "​",
            "style": "IPY_MODEL_ce4feb61be89413aadc62ee6c6539986",
            "value": " 733k/733k [00:00&lt;00:00, 22.8MB/s]"
          }
        },
        "e02760960d234936881bfe59d06b5e27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4c8d134c9bc47f2a6718a048c7b0a6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e43ff02af55f4a5b92556bf2c2cea202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fce2634169f14ba29d7c439d3c62a78d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e33082839c034083a05d82bd54bc538b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cb7dbe1abf2440da9fbe175aeb5e754": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce4feb61be89413aadc62ee6c6539986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20dd5b80ba3048889cdbfac3aa6683fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_617b8c771e0d4e128acf59faf15f3669",
              "IPY_MODEL_d19fd7d95a784b92b6331c6548dc1f65",
              "IPY_MODEL_860ac274d10d4795b9f71e59ba360c18"
            ],
            "layout": "IPY_MODEL_f99ab3b5f2784338bd485d637d5cdc85"
          }
        },
        "617b8c771e0d4e128acf59faf15f3669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30abdfd625ad4c9a93b74cd224af18dc",
            "placeholder": "​",
            "style": "IPY_MODEL_f85ccb60bd8d433a86457f8410bc3b13",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "d19fd7d95a784b92b6331c6548dc1f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_929148d6db4c4754825ae420f345a6a4",
            "max": 6357543,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d322e770d8c4e1d86e1a3feb53c8d60",
            "value": 6357543
          }
        },
        "860ac274d10d4795b9f71e59ba360c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5ddac7c0c5b4f818bba890a35e2e705",
            "placeholder": "​",
            "style": "IPY_MODEL_92199cef80e64eb8b86a2a4feb4532d1",
            "value": " 6.36M/6.36M [00:00&lt;00:00, 136MB/s]"
          }
        },
        "f99ab3b5f2784338bd485d637d5cdc85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30abdfd625ad4c9a93b74cd224af18dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f85ccb60bd8d433a86457f8410bc3b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "929148d6db4c4754825ae420f345a6a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d322e770d8c4e1d86e1a3feb53c8d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5ddac7c0c5b4f818bba890a35e2e705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92199cef80e64eb8b86a2a4feb4532d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1910a9940b3540ca915b8ccf92001341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8cbe9ee33334b8385c2a4099c3f91ea",
              "IPY_MODEL_f93d752f8dd843c099b532b5766fcfbf",
              "IPY_MODEL_2427315dfd2e4ef9935841010dbc8c24"
            ],
            "layout": "IPY_MODEL_c33feb6bff3f44a89a27502f7ee8bfa7"
          }
        },
        "c8cbe9ee33334b8385c2a4099c3f91ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e48362acb52642dfbd5dbcff9f2e4299",
            "placeholder": "​",
            "style": "IPY_MODEL_cb205572edf740588f46f61c166d86ed",
            "value": "validation-00000-of-00001.parquet: 100%"
          }
        },
        "f93d752f8dd843c099b532b5766fcfbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9487d987e3b43e7ba3e19c392cfab83",
            "max": 657209,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0405e5311e5746d9a204a099508a7630",
            "value": 657209
          }
        },
        "2427315dfd2e4ef9935841010dbc8c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21fad294bcd64f71b4b169aed557df6f",
            "placeholder": "​",
            "style": "IPY_MODEL_cb3d93a56a79444a9c6a8fb97ef3c708",
            "value": " 657k/657k [00:00&lt;00:00, 23.0MB/s]"
          }
        },
        "c33feb6bff3f44a89a27502f7ee8bfa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e48362acb52642dfbd5dbcff9f2e4299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb205572edf740588f46f61c166d86ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9487d987e3b43e7ba3e19c392cfab83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0405e5311e5746d9a204a099508a7630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21fad294bcd64f71b4b169aed557df6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb3d93a56a79444a9c6a8fb97ef3c708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c19bbb46c9b4ed286ac9f48042f3d63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80115cd1b26e44248e9c79e210614187",
              "IPY_MODEL_8e3c1dd1b7fe40039b3a7e3d5beb2f32",
              "IPY_MODEL_c2c25aca3ac8441185a54d856ce69ded"
            ],
            "layout": "IPY_MODEL_04a0b981d0b94386a0905c4615a5fa6d"
          }
        },
        "80115cd1b26e44248e9c79e210614187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee61bc449e9c4bb0a87ff3a6f43457cb",
            "placeholder": "​",
            "style": "IPY_MODEL_81832b82b0d64a9593b0c61780fde7a1",
            "value": "Generating test split: 100%"
          }
        },
        "8e3c1dd1b7fe40039b3a7e3d5beb2f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f36690f14594c8995e2f2a492c3d562",
            "max": 4358,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e8c0cce22904f0ea26f7caa1f388da5",
            "value": 4358
          }
        },
        "c2c25aca3ac8441185a54d856ce69ded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f55cac1eb7fe4e47951713f1d3e3bf77",
            "placeholder": "​",
            "style": "IPY_MODEL_9e6e1c04569c4eb9a0dc296eb836f258",
            "value": " 4358/4358 [00:00&lt;00:00, 60519.54 examples/s]"
          }
        },
        "04a0b981d0b94386a0905c4615a5fa6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee61bc449e9c4bb0a87ff3a6f43457cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81832b82b0d64a9593b0c61780fde7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f36690f14594c8995e2f2a492c3d562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e8c0cce22904f0ea26f7caa1f388da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f55cac1eb7fe4e47951713f1d3e3bf77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6e1c04569c4eb9a0dc296eb836f258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44da71af1b0148fd9b574b8b380db4db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84714516b80c4e1b9600eead05cde90f",
              "IPY_MODEL_76bd9710a7874c3fba3516da3fee1403",
              "IPY_MODEL_4f3ff1032b2049c4ac2aad967bead4ba"
            ],
            "layout": "IPY_MODEL_a838c60b7b5e4a06912c3f5adf61e75f"
          }
        },
        "84714516b80c4e1b9600eead05cde90f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a99321779b0b4df49bbaf9fafa8bd29c",
            "placeholder": "​",
            "style": "IPY_MODEL_3fb130a8d1734281add85f6da31d1c1d",
            "value": "Generating train split: 100%"
          }
        },
        "76bd9710a7874c3fba3516da3fee1403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b961d4752e4ca09bc0ca75d2383a45",
            "max": 36718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c99e66205a544aeb8077a4eb40ee486",
            "value": 36718
          }
        },
        "4f3ff1032b2049c4ac2aad967bead4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6190a548736a41fb8093b234d6528686",
            "placeholder": "​",
            "style": "IPY_MODEL_8b105ce09f774e52bba03edb361cf40c",
            "value": " 36718/36718 [00:00&lt;00:00, 349511.85 examples/s]"
          }
        },
        "a838c60b7b5e4a06912c3f5adf61e75f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a99321779b0b4df49bbaf9fafa8bd29c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fb130a8d1734281add85f6da31d1c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5b961d4752e4ca09bc0ca75d2383a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c99e66205a544aeb8077a4eb40ee486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6190a548736a41fb8093b234d6528686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b105ce09f774e52bba03edb361cf40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdd94f782a5244a095f10acd0a742117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9049d2da9fa400d9c5092e62cd35c86",
              "IPY_MODEL_369724ac1ab44801930e216912ca9c1f",
              "IPY_MODEL_bbb55d16ae724b1aa1e27499a0c8752a"
            ],
            "layout": "IPY_MODEL_a3516f75844f4f8a894c061a90b4749a"
          }
        },
        "b9049d2da9fa400d9c5092e62cd35c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b6176ca996e4be5a0a95aabf9b36d30",
            "placeholder": "​",
            "style": "IPY_MODEL_7df6e55b837144cb9943dcf7e0a22a8e",
            "value": "Generating validation split: 100%"
          }
        },
        "369724ac1ab44801930e216912ca9c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8960bb99a214131a96491eb9b819bd6",
            "max": 3760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_203c157caecc4a4d9058e093b8ba571d",
            "value": 3760
          }
        },
        "bbb55d16ae724b1aa1e27499a0c8752a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5595138fc9de48ed872cd60f74191e16",
            "placeholder": "​",
            "style": "IPY_MODEL_6f7683e4ffa94b80bd05d622cf1d476c",
            "value": " 3760/3760 [00:00&lt;00:00, 134172.05 examples/s]"
          }
        },
        "a3516f75844f4f8a894c061a90b4749a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b6176ca996e4be5a0a95aabf9b36d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df6e55b837144cb9943dcf7e0a22a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8960bb99a214131a96491eb9b819bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "203c157caecc4a4d9058e093b8ba571d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5595138fc9de48ed872cd60f74191e16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f7683e4ffa94b80bd05d622cf1d476c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqkdKB4cVZub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d46f7d9-e040-4fea-ccd0-9f9cf3e6d069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Tokenization (15 pts)\n",
        "\n",
        "How does one represent textual input to language models? One strategy that we have seen is to split up words on spaces, e.g.,\n",
        "\n",
        "> This is an example.\n",
        "\n",
        "> [This, is, an, example],\n",
        "\n",
        "but this fails when unseen words appear at test time, e.g.,\n",
        "\n",
        "> We named our son nwonkun.\n",
        "\n",
        "> [We, named, our, son, \\<unk\\>] (5 tokens).\n",
        "\n",
        "One solution to this problem is to use character-level tokens\n",
        "\n",
        "> [W, e, _, n, a, m, e, d, _, o, u, r, _, s, o, n, _, n, w, o, n, k, u, n]\n",
        "\n",
        "(24 tokens, if I counted right), but now the number of tokens required to encode a sentence has increased a *lot*."
      ],
      "metadata": {
        "id": "vRuwwGP_Vaca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Byte-pair encodings and sub-word tokenization (5 pts)\n",
        "\n",
        "[Byte-pair encodings (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) are a clever middle ground for the tokenization problem.\n",
        "Starting from a character-level tokenization, iteratively combine the most common bigrams (token pairs) into their own tokens.\n",
        "For example, the most common bigrams from the previous example are \"_n\" and \"on\". Breaking the tie arbitrarily and creating a new token \"_n\" we now have\n",
        "\n",
        "> [W, e, _n, a, m, e, d, _, o, u, r, _, s, o, n, _n, w, o, n, k, u, n]\n",
        "\n",
        "reducing the token count to 22. Iteratively applying this rule, we can further reduce it to 20 tokens by adding the token \"on\", and so on. Each step of this algorithm greedily reduces the token count by the maximum amount.\n",
        "\n",
        "This tokenization scheme, known as \"sub-word tokenization\" takes the best of both worlds: since the vocabulary still contains tokens for every byte, we never have to use the \\<unk\\> token, while still reducing the number of required tokens to encode a sequence. The more tokens you add, the shorter your sequence gets.\n",
        "\n",
        "To decide which tokens to add to the vocabulary, we have to *train* our BPE tokenizer on a corpus.\n",
        "In this section you will do just that."
      ],
      "metadata": {
        "id": "wr-8Wi_n0HUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "train: str = str.join(\" \", dataset[\"train\"][\"text\"])[:pow(10, 6)]\n",
        "test: str = str.join(\" \", dataset[\"test\"][\"text\"])[:pow(10, 6)]"
      ],
      "metadata": {
        "id": "2AhiO_ZdaHR6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345,
          "referenced_widgets": [
            "b21c569c70e44f4797d3253c372adc62",
            "4d645581d43f4266991b2e557e508106",
            "9acff515db1540de966679a200a6d513",
            "d7da60183a55497fb94007b5dcfd5d71",
            "e40bcf2027384917aa5e20f0a687909d",
            "78edb5011e164476997f813ff0327e47",
            "f98b71c024e348d68898b07e5def8281",
            "27d57fefd4c14f97bc419929ff9935d8",
            "f55ace0efe2a4014a9ece20581943ef0",
            "63629e816d964dceac6146a250321a46",
            "ca7dd20ddbc94cc39893f642afd4dc73",
            "f46e980c454a45fc90ea6b59cc779261",
            "222ebfa12b7444b1afc39d1b1be81243",
            "89d555269dd4452081e80029c477b305",
            "a0d446a517444e0ba5128cd2ae0ed757",
            "e02760960d234936881bfe59d06b5e27",
            "e4c8d134c9bc47f2a6718a048c7b0a6b",
            "e43ff02af55f4a5b92556bf2c2cea202",
            "fce2634169f14ba29d7c439d3c62a78d",
            "e33082839c034083a05d82bd54bc538b",
            "6cb7dbe1abf2440da9fbe175aeb5e754",
            "ce4feb61be89413aadc62ee6c6539986",
            "20dd5b80ba3048889cdbfac3aa6683fb",
            "617b8c771e0d4e128acf59faf15f3669",
            "d19fd7d95a784b92b6331c6548dc1f65",
            "860ac274d10d4795b9f71e59ba360c18",
            "f99ab3b5f2784338bd485d637d5cdc85",
            "30abdfd625ad4c9a93b74cd224af18dc",
            "f85ccb60bd8d433a86457f8410bc3b13",
            "929148d6db4c4754825ae420f345a6a4",
            "0d322e770d8c4e1d86e1a3feb53c8d60",
            "b5ddac7c0c5b4f818bba890a35e2e705",
            "92199cef80e64eb8b86a2a4feb4532d1",
            "1910a9940b3540ca915b8ccf92001341",
            "c8cbe9ee33334b8385c2a4099c3f91ea",
            "f93d752f8dd843c099b532b5766fcfbf",
            "2427315dfd2e4ef9935841010dbc8c24",
            "c33feb6bff3f44a89a27502f7ee8bfa7",
            "e48362acb52642dfbd5dbcff9f2e4299",
            "cb205572edf740588f46f61c166d86ed",
            "f9487d987e3b43e7ba3e19c392cfab83",
            "0405e5311e5746d9a204a099508a7630",
            "21fad294bcd64f71b4b169aed557df6f",
            "cb3d93a56a79444a9c6a8fb97ef3c708",
            "7c19bbb46c9b4ed286ac9f48042f3d63",
            "80115cd1b26e44248e9c79e210614187",
            "8e3c1dd1b7fe40039b3a7e3d5beb2f32",
            "c2c25aca3ac8441185a54d856ce69ded",
            "04a0b981d0b94386a0905c4615a5fa6d",
            "ee61bc449e9c4bb0a87ff3a6f43457cb",
            "81832b82b0d64a9593b0c61780fde7a1",
            "0f36690f14594c8995e2f2a492c3d562",
            "0e8c0cce22904f0ea26f7caa1f388da5",
            "f55cac1eb7fe4e47951713f1d3e3bf77",
            "9e6e1c04569c4eb9a0dc296eb836f258",
            "44da71af1b0148fd9b574b8b380db4db",
            "84714516b80c4e1b9600eead05cde90f",
            "76bd9710a7874c3fba3516da3fee1403",
            "4f3ff1032b2049c4ac2aad967bead4ba",
            "a838c60b7b5e4a06912c3f5adf61e75f",
            "a99321779b0b4df49bbaf9fafa8bd29c",
            "3fb130a8d1734281add85f6da31d1c1d",
            "b5b961d4752e4ca09bc0ca75d2383a45",
            "1c99e66205a544aeb8077a4eb40ee486",
            "6190a548736a41fb8093b234d6528686",
            "8b105ce09f774e52bba03edb361cf40c",
            "fdd94f782a5244a095f10acd0a742117",
            "b9049d2da9fa400d9c5092e62cd35c86",
            "369724ac1ab44801930e216912ca9c1f",
            "bbb55d16ae724b1aa1e27499a0c8752a",
            "a3516f75844f4f8a894c061a90b4749a",
            "2b6176ca996e4be5a0a95aabf9b36d30",
            "7df6e55b837144cb9943dcf7e0a22a8e",
            "a8960bb99a214131a96491eb9b819bd6",
            "203c157caecc4a4d9058e093b8ba571d",
            "5595138fc9de48ed872cd60f74191e16",
            "6f7683e4ffa94b80bd05d622cf1d476c"
          ]
        },
        "outputId": "ae378467-d368-47b2-b02e-72a8bf6fd820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b21c569c70e44f4797d3253c372adc62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f46e980c454a45fc90ea6b59cc779261"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20dd5b80ba3048889cdbfac3aa6683fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1910a9940b3540ca915b8ccf92001341"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c19bbb46c9b4ed286ac9f48042f3d63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44da71af1b0148fd9b574b8b380db4db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdd94f782a5244a095f10acd0a742117"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain, pairwise\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Tokenizer:\n",
        "    # The lookup list contains *byte groups*, represented as a tuples of ints.\n",
        "    # The token ID for a byte group is its index in the list.\n",
        "    vocab: list[tuple[int, ...]]\n",
        "\n",
        "    def __init__(self, training_seq: str, vocab_size: int) -> None:\n",
        "        # Initialize a lookup with single-byte groups\n",
        "        self.vocab = [(i,) for i in range(pow(2, 8))]\n",
        "        byte_seq = list(bytes(training_seq, \"utf-8\"))\n",
        "        for i in tqdm(range(pow(2, 8), vocab_size)):\n",
        "            \"\"\"\n",
        "            TODO: iteratively add the most common token pairs to the vocabulary.\n",
        "            Advice: try using Counter and pairwise from the python std lib.\n",
        "            \"\"\"\n",
        "            pairs = pairwise(byte_seq)\n",
        "            counts = Counter(pairs)\n",
        "\n",
        "            if not counts:\n",
        "                break\n",
        "\n",
        "            common, _ = counts.most_common(1)[0]\n",
        "\n",
        "            if all(0 <= b < 256 for b in common):\n",
        "                self.vocab.append(common)\n",
        "            #else:\n",
        "            #    print(f\"Skipping invalid pair: {common}\")\n",
        "\n",
        "            hu = []\n",
        "            i = 0\n",
        "            while i < len(byte_seq):\n",
        "                if i < len(byte_seq) - 1 and (byte_seq[i], byte_seq[i + 1]) == common:\n",
        "                    hu.append(len(self.vocab) - 1)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    hu.append(byte_seq[i])\n",
        "                    i += 1\n",
        "            byte_seq = hu\n",
        "\n",
        "    def tokenize(self, seq: str) -> list[int]:\n",
        "        \"\"\"\n",
        "        TODO: convert a byte sequence into a token sequence by greedily adding\n",
        "        the longest token that matches the rest of the sequence, e.g.,\n",
        "        vocab = [a, aa, b]\n",
        "        sequence = aaab\n",
        "        token_seq = [1, 0, 2] NOT [0, 1, 2].\n",
        "        \"\"\"\n",
        "        byte_seq = list(bytes(seq, \"utf-8\"))\n",
        "        token_seq = []\n",
        "\n",
        "        i = 0\n",
        "        while i < len(byte_seq):\n",
        "            for length in range(len(self.vocab), 0, -1):\n",
        "                token = tuple(byte_seq[i:i + length])\n",
        "                if token in self.vocab:\n",
        "                    token_seq.append(self.vocab.index(token))\n",
        "                    i += length\n",
        "                    break\n",
        "            else:\n",
        "                raise ValueError(\"No matching token found\")\n",
        "        return token_seq\n",
        "\n",
        "    def detokenize(self, token_seq: list[int]) -> str:\n",
        "        # TODO: convert a token sequence into a byte sequence.\n",
        "        byte_seq = list(chain.from_iterable(self.vocab[token] for token in token_seq))\n",
        "        return bytes(byte_seq).decode(\"utf-8\")\n",
        "\n",
        "train_data = train[:10000]\n",
        "bigrams = list(pairwise(bytes(train_data, \"utf-8\")))\n",
        "bigram_counts = Counter(bigrams)\n",
        "print(\"Most common bigrams in the training data:\")\n",
        "print(bigram_counts.most_common(10))\n",
        "tokenizer = Tokenizer(train_data, vocab_size=500)\n",
        "\n",
        "print(\"Some of our new tokens:\")\n",
        "for token in tokenizer.vocab[-10:]:\n",
        "    print(repr(bytes(token).decode(\"utf-8\")))"
      ],
      "metadata": {
        "id": "PQPhCEYWaL5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4406c521-d409-4cc4-8321-542271e7507e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common bigrams in the training data:\n",
            "[((101, 32), 301), ((32, 116), 249), ((115, 32), 231), ((116, 104), 203), ((104, 101), 173), ((32, 97), 167), ((100, 32), 152), ((110, 32), 145), ((101, 114), 139), ((97, 110), 124)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 244/244 [00:00<00:00, 249.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some of our new tokens:\n",
            "'ga'\n",
            "'be'\n",
            "'su'\n",
            "'ki'\n",
            "'ug'\n",
            "'as'\n",
            "'av'\n",
            "'ul'\n",
            "'bo'\n",
            "'we'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a sanity check, your implementation should be able to compress the training set to ~40-50% of its original size.\n",
        "You should notice that the test set compression does not perform as well. This is because the distribution of bigrams in the test set does not exactly match the that of the train set. This gets worse the further your test set distribution is from your training set."
      ],
      "metadata": {
        "id": "psiYF10PELk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not edit this code cell\n",
        "test_data = test[:10000]\n",
        "train_bytes_len = len(bytes(train_data, \"utf-8\"))\n",
        "train_token_len = len(tokenizer.tokenize(train_data))\n",
        "print(f\"Compressed train set to {train_token_len / train_bytes_len * 100:.0f}% original size\")\n",
        "test_bytes_len = len(bytes(test_data, \"utf-8\"))\n",
        "test_token_len = len(tokenizer.tokenize(test_data))\n",
        "print(f\"Compressed test set to {test_token_len / test_bytes_len * 100:.0f}% original size\")\n",
        "\n",
        "assert train_data == tokenizer.detokenize(tokenizer.tokenize(train_data))\n",
        "assert test_data == tokenizer.detokenize(tokenizer.tokenize(test_data))"
      ],
      "metadata": {
        "id": "osy9s9wiD2A1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef79d928-dacc-41ca-9a67-be9729193495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compressed train set to 63% original size\n",
            "Compressed test set to 66% original size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 BPE performance on OOD text. (5 pts)\n",
        "\n",
        "Explore how English-trained BPE performs on non-English text by downloading corpora from a few different languages and using your English-trained tokenizer. What do you find? Do the results match your expectations? For what langauges does the tokenizer struggle with the most? How might this impact society if everyone were to use your tokenizer?\n",
        "\n",
        "Include your code, results, and discussion in new cells below.\n",
        "\n",
        "Hint: we recommend you use `load_dataset` to fetch from HuggingFace with `streaming=True` to avoid huge downloads. You might want to take a look at the `oscar` dataset."
      ],
      "metadata": {
        "id": "ZrvZyfOvFqNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "\n",
        "langs = [\"fr\", \"es\", \"zh\", \"ar\"]\n",
        "corp = {}\n",
        "\n",
        "for i in langs:\n",
        "    dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{i}\", streaming=True, trust_remote_code=True)\n",
        "    corp[i] = \" \".join(item[\"text\"] for item in islice(dataset[\"train\"], 1000))\n",
        "\n",
        "for i, corpus in corp.items():\n",
        "    td = corpus[:10000]\n",
        "    tl = len(tokenizer.tokenize(td))\n",
        "    tb = len(bytes(td, \"utf-8\"))\n",
        "\n",
        "    print(f\" {i} = {tl / tb * 100:.0f}% original size\")\n",
        "    sample = tokenizer.tokenize(td[:200])\n",
        "    #print(f\"Sample : {sample}\")\n",
        "    detokenized_sample = tokenizer.detokenize(sample)\n",
        "    #print(f\"detokenized : {detokenized_sample}\")\n"
      ],
      "metadata": {
        "id": "-F_CpvPjHdX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faf1b78c-7cc4-49dc-8258-f85ce6de3213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " fr = 74% original size\n",
            " es = 68% original size\n",
            " zh = 100% original size\n",
            " ar = 99% original size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Pitfalls of and alternatives to BPE (5 pts)\n",
        "\n",
        "BPE tokenization sufferes from other issues as well. Due to the implementation of our BPE tokenizer, detokenizing a sequence of tokens then re-tokenizing it does not always recover the original sequence:\n",
        "```\n",
        "vocab = {a, aa, b}\n",
        "tokens = [0, 1, 2]\n",
        "detokenized = aaab\n",
        "retokenized = [1, 0, 2]\n",
        "```\n",
        "\n",
        "Another issue is that some tokens that may have been prevalent during BPE training may not be present during language model training, leading to funky situations where the language model has not been trained to represent or output some tokens. See this paper for more information: https://arxiv.org/pdf/2405.05417.\n",
        "\n",
        "Some NLP researchers think that we should move away from sub-word tokenization to get rid of these problems. Engage with this discussion by either\n",
        "- Finding a paper that points out an issue with tokenization and propose your own solution for how you would fix it, or\n",
        "- Finding a paper that proposes an alternative tokenization scheme (or way of processing text) and discuss the drawbacks of the proposed method.\n",
        "\n",
        "Your response should be about a paragraph in length and link to a paper."
      ],
      "metadata": {
        "id": "4wc6vsHwKtiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This paper titled Byte Pair Encoding is Suboptimal for Language Model Pretraining (https://arxiv.org/pdf/2004.03720) addresses the issues associated with BPE. One issue is that it merges/splits words in a manner that may not be contextually correct, such as words with vital latin roots that it could miss ( something like Universal). Words that commonly appear in threes cause a pairing between two words and the third intermediate token will be a standalone in the vocabularily thats rarely used. BPE also faces a problem where common affixes are merged with other tokens that they shouldn't be, making it sound less coherent.**"
      ],
      "metadata": {
        "id": "bY9nKjwwxQAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Generation Algorithms (35 pts + 15 pts BONUS)\n",
        "\n",
        "In this problem, we will implement several common decoding algorithms and test them with the GPT-2 Medium model.\n",
        "\n",
        "Given the class below, we will fill in each of the method stubs. You may create additional helper methods as well to make components re-usable.\n",
        "\n",
        "**You are not allowed to use the generate() function in the transformers library. You can only use the model's forward() method to retrieve final layer logits**\n",
        "\n",
        "In addition to the methods we ask you to implement, which are:\n",
        "- Greedy decoding\n",
        "- Temperature Sampling\n",
        "- Nucleus Sampling\n",
        "\n",
        "You will choose ONE of the following sampling algorithms to implement as well (make sure to add your own method, since we do not provide one by default):\n",
        "- Typical Sampling ([Meister et al. (2022)](https://arxiv.org/abs/2202.00666))\n",
        "- Eta Sampling ([Hewitt et al. (2022)](https://arxiv.org/abs/2210.15191))\n",
        "\n",
        "Points for this question will be distributed as follows:\n",
        "\n",
        "- 5-10 points for implementing each decoding algorithm\n",
        "- 5 points for implementing the generate() function (you will make this incrementally through each sub-part)\n",
        "- 5 points for filling out the table with list of tokens (see instructions below)"
      ],
      "metadata": {
        "id": "xxgYpTuExIv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import Optional\n",
        "\n",
        "class LM():\n",
        "  def __init__(self, model_name: str = \"openai-community/gpt2-medium\"):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    self.model.eval()\n",
        "\n",
        "  def greedy_decoding(self, prompt: str, max_length: int = 64) -> str:\n",
        "    \"\"\"\n",
        "    TODO:\n",
        "\n",
        "    Implement greedy decoding, in which we use the highest\n",
        "    probability token at each decoding step\n",
        "    \"\"\"\n",
        "    ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    out = ids.clone()\n",
        "\n",
        "    for x in range(max_length):\n",
        "        logits = self.model(input_ids=out).logits\n",
        "        nids = torch.argmax(logits[:, -1, :], dim=-1)\n",
        "        out = torch.cat([out, nids.unsqueeze(-1)], dim=-1)\n",
        "        if nids.item() == self.tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "  def temperature_sampling(self, prompt: str, temperature: float = 1.0, max_length: int = 64) -> str:\n",
        "    \"\"\"\n",
        "    TODO:\n",
        "\n",
        "    Implement temperature sampling, in which we sample\n",
        "    from the output distribution at each decoding step,\n",
        "    with a temperature parameter to control the \"peakiness\"\n",
        "    of the output distribution\n",
        "    \"\"\"\n",
        "    ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    out = ids.clone()\n",
        "\n",
        "    for x in range(max_length):\n",
        "        logits = self.model(input_ids=out).logits\n",
        "        scaled_logits = logits[:, -1, :] / temperature\n",
        "        probs = torch.softmax(scaled_logits, dim=-1)\n",
        "        nids = torch.multinomial(probs, num_samples=1)\n",
        "        out = torch.cat([out, nids], dim=-1)\n",
        "        if nids.item() == self.tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "  def nucleus_sampling(self, prompt: str, p: float = 0.9, max_length: int = 64, temperature: float = 1.0) -> str:\n",
        "    \"\"\"\n",
        "    TODO:\n",
        "    Implement nucleus sampling, in which we\n",
        "    sample from a subset of the vocabulary\n",
        "    at each decoding step\n",
        "    Note: There is also a temperature parameter here\n",
        "    \"\"\"\n",
        "    ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    out = ids.clone()\n",
        "\n",
        "    for x in range(max_length):\n",
        "        logits = self.model(input_ids=out).logits\n",
        "        scaled_logits = logits[:, -1, :] / temperature\n",
        "        probs = torch.softmax(scaled_logits, dim=-1)\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "        allprobs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        inRange = allprobs <= p\n",
        "        if not inRange.any():\n",
        "            nids = sorted_indices[0, 0].unsqueeze(0)\n",
        "        else:\n",
        "            nucleus_indices = sorted_indices[inRange]\n",
        "            nucleus_probs = sorted_probs[inRange]\n",
        "\n",
        "            nids = nucleus_indices[torch.multinomial(nucleus_probs, num_samples=1)]\n",
        "\n",
        "        out = torch.cat([out, nids.unsqueeze(0)], dim=-1)\n",
        "        if nids.item() == self.tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "  def typical_sampling(self, prompt: str, t: float = 0.9, max_length: int = 64, temperature: float = 1.0) -> str:\n",
        "\n",
        "    ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    out = ids.clone()\n",
        "\n",
        "    for x in range(max_length):\n",
        "        logits = self.model(input_ids=out).logits\n",
        "        scaled_logits = logits[:, -1, :] / temperature\n",
        "        probs = torch.softmax(scaled_logits, dim=-1)\n",
        "\n",
        "        entropy = -(probs * probs.log()).sum(dim=-1, keepdim=True)\n",
        "        conv = -probs.log() / entropy\n",
        "        sorted_conv, sorted_indices = torch.sort(torch.abs(conv - t), descending=False)\n",
        "        truncated_probs = probs[0, sorted_indices[0]]\n",
        "\n",
        "        normalized_probs = truncated_probs / truncated_probs.sum()\n",
        "        sampled_index = torch.multinomial(normalized_probs, num_samples=1).item()\n",
        "        next_token = (sorted_indices[0][sampled_index]).unsqueeze(0).unsqueeze(0)\n",
        "        out = torch.cat([out, next_token], dim=-1)\n",
        "\n",
        "        if next_token.item() == self.tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    return self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "  def generate(self,\n",
        "               prompt: str,\n",
        "               temperature: float = 1.0,\n",
        "               p: Optional[float] = None) -> str:\n",
        "      \"\"\"\n",
        "      TODO:\n",
        "\n",
        "      Route to the appropriate generation function\n",
        "      based on the arguments\n",
        "      HINT: What parameter values should map to greedy decoding?\n",
        "      \"\"\"\n",
        "      max_length = 64\n",
        "      if p is not None:\n",
        "          return self.nucleus_sampling(prompt, p=p, temperature=temperature, max_length=max_length)\n",
        "      elif temperature < 0.5:\n",
        "          return self.greedy_decoding(prompt, max_length=max_length)\n",
        "      else:\n",
        "          return self.temperature_sampling(prompt, temperature=temperature, max_length=max_length)\n",
        "\n",
        "\n",
        "  def greedyTop10(self, prompt: str) -> list[tuple[str, float]]:\n",
        "      input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "      logits = self.model(input_ids=input_ids).logits[:, -1, :]\n",
        "      topkprob, topk = torch.topk(torch.softmax(logits, dim=-1), 10)\n",
        "\n",
        "      tokens = []\n",
        "      for prob, idx in zip(topkprob[0], topk[0]):\n",
        "          try:\n",
        "              decoded = self.tokenizer.decode([idx])\n",
        "              tokens.append((decoded, prob.item()))\n",
        "          except UnicodeDecodeError:\n",
        "              continue\n",
        "\n",
        "      return tokens\n",
        "\n",
        "  def typicalTop10(self, prompt: str, t: float = 0.9, temperature: float = 1.0) -> list[tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Implements Typical Sampling's top 10 tokens at the first decoding step.\n",
        "        \"\"\"\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "        logits = self.model(input_ids=input_ids).logits[:, -1, :] / temperature\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        entropy = -(probs * probs.log()).sum(dim=-1, keepdim=True)\n",
        "        conv = -probs.log() / entropy\n",
        "        deviation = torch.abs(conv - t).squeeze()\n",
        "        sorted_indices = torch.argsort(deviation)\n",
        "\n",
        "        top10i = sorted_indices[:10]\n",
        "        tenprob = probs[0, top10i]\n",
        "        tentoken = [(self.tokenizer.decode(idx), prob.item()) for idx, prob in zip(top10i, tenprob)]\n",
        "\n",
        "        return tentoken\n",
        "\n",
        "  def tempTop10(self, prompt: str, temperature: float = 1.0) -> list[tuple[str, float]]:\n",
        "      input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "      logits = self.model(input_ids=input_ids).logits[:, -1, :] / temperature\n",
        "      topkprob, topk = torch.topk(torch.softmax(logits, dim=-1), 10)\n",
        "\n",
        "      tokens = []\n",
        "      for prob, idx in zip(topkprob[0], topk[0]):\n",
        "          decoded = self.tokenizer.decode([idx])\n",
        "          if decoded.strip():\n",
        "              tokens.append((decoded, prob.item()))\n",
        "      return tokens\n",
        "\n",
        "  def nucleusTop10(self, prompt: str, p: float = 0.9, temperature: float = 1.0) -> list[tuple[str, float]]:\n",
        "      input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "      logits = self.model(input_ids=input_ids).logits[:, -1, :] / temperature\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "\n",
        "      inRange = (torch.cumsum(sorted_probs, dim=-1) <= p)\n",
        "      topkprob = sorted_probs[inRange][:10]\n",
        "      topk = sorted_indices[inRange][:10]\n",
        "\n",
        "      tokens = []\n",
        "      for prob, idx in zip(topkprob, topk):\n",
        "          decoded = self.tokenizer.decode([idx])\n",
        "          if decoded.strip():\n",
        "              tokens.append((decoded, prob.item()))\n",
        "      return tokens\n",
        "\n",
        "\n",
        "lm = LM()\n",
        "prompt = \"Once upon a time in a land far far away,\"\n",
        "\n",
        "print(\"greedy:\", lm.greedyTop10(prompt))\n",
        "print(\"temperature (t=1.0):\", lm.tempTop10(prompt))\n",
        "print(\"nucleus (p=0.9):\", lm.nucleusTop10(prompt))\n",
        "print(\"typical sampling:\", lm.typicalTop10(prompt, t=0.9, temperature=1.0))"
      ],
      "metadata": {
        "id": "BR11jNsRqAw3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f7c483-e0b4-4b10-d472-c50256265114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "greedy: [(' there', 0.21611085534095764), (' a', 0.16582567989826202), (' the', 0.06953583657741547), (' in', 0.03848272189497948), ('\\n', 0.03528774157166481), (' I', 0.0322796106338501), (' an', 0.01550489105284214), (' when', 0.009391860105097294), (' two', 0.009228527545928955), (' you', 0.009130480699241161)]\n",
            "temperature (t=1.0): [(' there', 0.21611085534095764), (' a', 0.16582567989826202), (' the', 0.06953583657741547), (' in', 0.03848272189497948), (' I', 0.0322796106338501), (' an', 0.01550489105284214), (' when', 0.009391860105097294), (' two', 0.009228527545928955), (' you', 0.009130480699241161)]\n",
            "nucleus (p=0.9): [(' there', 0.21611085534095764), (' a', 0.16582567989826202), (' the', 0.06953583657741547), (' in', 0.03848272189497948), (' I', 0.0322796106338501), (' an', 0.01550489105284214), (' when', 0.009391860105097294), (' two', 0.009228527545928955), (' you', 0.009130480699241161)]\n",
            "typical sampling: [(' an', 0.01550489105284214), (' when', 0.009391860105097294), (' two', 0.009228527545928955), (' you', 0.009130480699241161), (' I', 0.0322796106338501), (' many', 0.007748258300125599), ('\\n', 0.03528774157166481), (' in', 0.03848272189497948), (' my', 0.006503292825073004), (' one', 0.006039347033947706)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT2LM = LM(\"openai-community/gpt2-medium\")"
      ],
      "metadata": {
        "id": "ibdqPky8aCXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each sampling algorithm you implement, fill out this table, in which you will list the top 10 highest probability tokens **at the first decoding step** in a comma separated list. For algorithms like nucleus sampling where you perform some kind of truncation/re-distribution of the output distribution, do the truncation/re-distribution first, and then sort the vocabulary by probability to complete the table.\n",
        "\n",
        "For this and all questions below, use the following prompt:\n",
        "\n",
        "\n",
        "**\"Once upon a time in a land far far away, \"**\n",
        "\n",
        "Note: Use the default value for `max_length` for all questions below."
      ],
      "metadata": {
        "id": "9pJTajqLSq76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Decoding Algorithm** | **10 Highest Probability Tokens** |\n",
        "|------------------------|-----------------------------------|\n",
        "| Greedy                 | [(' there', 0.21611085534095764), (' a', 0.16582567989826202), (' the', 0.06953583657741547), (' in', 0.03848272189497948), ('\\n', 0.03528774157166481), (' I', 0.0322796106338501), (' an', 0.01550489105284214), (' when', 0.009391860105097294), (' two', 0.009228527545928955), (' you', 0.009130480699241161)] |\n",
        "| Temperature (t=1.0)    | [(' there', 0.21611085534095764), (' a', 0.16582567989826202), (' the', 0.06953583657741547), (' in', 0.03848272189497948), ('\\n', 0.03528774157166481), (' I', 0.0322796106338501), (' an', 0.01550489105284214), (' when', 0.009391860105097294), (' two', 0.009228527545928955), (' you', 0.009130480699241161)]  |\n",
        "| Nucleus (p=0.9)        | [(' there', 0.21611085534095764), (' a', 0.16582567989826202), (' the', 0.06953583657741547), (' in', 0.03848272189497948), ('\\n', 0.03528774157166481), (' I', 0.0322796106338501), (' an', 0.01550489105284214), (' when', 0.009391860105097294), (' two', 0.009228527545928955), (' you', 0.009130480699241161)]|\n",
        "| Typical/Eta            | [(' an', 0.01550489105284214), (' when', 0.009391860105097294), (' two', 0.009228527545928955), (' you', 0.009130480699241161), (' I', 0.0322796106338501), (' many', 0.007748258300125599), ('\\n', 0.03528774157166481), (' in', 0.03848272189497948), (' my', 0.006503292825073004), (' one', 0.006039347033947706)]|"
      ],
      "metadata": {
        "id": "t7tJ9bQoTQ8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Greedy Decoding (5 points)\n",
        "\n",
        "First, implement the most simple decoding method of greedy decoding. Here, at each decoding time step, simply use the highest probability token. Note that you'll need to adjust the generate function so that a specific temperature value will map to greedy decoding (what should that value be?).\n",
        "\n",
        "Use the prompt given above to test your implementation. What do you notice?"
      ],
      "metadata": {
        "id": "OpV48G_ez8xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = LM()\n",
        "prompt = \"Once upon a time in a land far far away,\"\n",
        "output = lm.greedy_decoding(prompt)\n",
        "print(\"Greedy output:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F55HDHVAA2_a",
        "outputId": "65122dfa-f7ad-4214-aec5-618f910a8255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy output: Once upon a time in a land far far away, there lived a man named Simeon. He was a wise man, and he knew the secrets of the universe. He knew that the universe was made of many worlds, and that each world was made of a single substance. He knew that each substance was made of a single substance, and that each substance was made\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Greedy Decoding Output: Once upon a time in a land far far away,  a young man named   was born.  He was a   tall,   beautiful,   and   strong  man.  He was also a   very   strong  man.  He was also a**   \n",
        "\n",
        "**I notice it's very repetitive, and doesn't sound very fluent**"
      ],
      "metadata": {
        "id": "iTuogAKRYFPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 Temperature Sampling (10 pts)\n",
        "\n",
        "Sometimes (a lot of the time?), we don't actually just want the highest probability token at each time step. Why might this be the case?"
      ],
      "metadata": {
        "id": "xaKx8iVHYZjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is because using the most likely words every time will limit the scope of your language, and you will be using the same words repetitively since they will all have high probabilities with each other**"
      ],
      "metadata": {
        "id": "lH_L6QFjYhzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To adjust for this, we often use sampling algorithms instead of greedy decoding. However, there are many ways we can go about sampling.\n",
        "\n",
        "First, implement temperature sampling. Recall that the temperature parameter adjusts the \"randomness\" of the output at each time step. Here, you'll need to think about how to adjust the output distribution which you will do multinomial sampling from. Be careful about how you will handle very low (close to 0) temperatures.\n",
        "\n",
        "Given the same prompt as above, test your implementation with the following temperature values: [0.3, 0.5, 0.7, 0.9, 1.1]. For each value, sample 3 outputs. What do you notice in terms of the differences between output sets across different temperature values?  "
      ],
      "metadata": {
        "id": "WtwUy0ckIv79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = LM()\n",
        "prompt = \"Once upon a time in a land far far away,\"\n",
        "output = lm.temperature_sampling(prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "hxI1MULvJppY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f41f87-c569-4dd4-a9f2-7cdfcc7e927b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a land far far away, in a land far far away, I once owned property with no workers compensated pension or emergency funds and I had a lady. And she worked for eight years and there was one principal who would develop out of her right arm when she wanted cement chips.\" (comment from The Graham Hancock Show until the early 90s) I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temperatures = [0.3, 0.5, 0.7, 0.9, 1.1]\n",
        "results = {}\n",
        "\n",
        "prompt = \"Once upon a time in a land far far away,\"\n",
        "lm = LM()\n",
        "\n",
        "for temp in temperatures:\n",
        "    output = lm.generate(prompt, temperature=temp)\n",
        "    results[temp] = {\"Output\": output}\n",
        "\n",
        "for temp, data in results.items():\n",
        "    print(f\"Temperature: {temp}\")\n",
        "    print(f\"Generated Output: {data['Output']}\")\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4Lo3WiPYSlY",
        "outputId": "b5af3b99-dd61-4009-b419-8f88d4fda398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature: 0.3\n",
            "Generated Output: Once upon a time in a land far far away, there lived a man named Simeon. He was a wise man, and he knew the secrets of the universe. He knew that the universe was made of many worlds, and that each world was made of a single substance. He knew that each substance was made of a single substance, and that each substance was made\n",
            "\n",
            "\n",
            "\n",
            "Temperature: 0.5\n",
            "Generated Output: Once upon a time in a land far far away, in a land far away, some people lived in a cave. There were no trees or shrubs. There were only rocks and rocks and rocks. And the rocks were red. The rocks were red. And the rocks were red. And the rocks were red. And the rocks were red. And the rocks were red\n",
            "\n",
            "\n",
            "\n",
            "Temperature: 0.7\n",
            "Generated Output: Once upon a time in a land far far away, I was told that the greatest of all fates was to befall me, and that he who should stand before me should encounter an unseen enemy whom I could never hope to defeat. I was told that I was to be the first to fall, and that the next to die would be my brother. One day,\n",
            "\n",
            "\n",
            "\n",
            "Temperature: 0.9\n",
            "Generated Output: Once upon a time in a land far far away, there was a man with beautiful white hair, even though he was elf-like. They called him 'The Master'.\" [57]\n",
            "\n",
            "Despite the monstrous full moon until many years after the events of the LotR, a portion of the lore centers around the fact that the treacherous Khazad-dûm\n",
            "\n",
            "\n",
            "\n",
            "Temperature: 1.1\n",
            "Generated Output: Once upon a time in a land far far away, There lived a nobleman When all minds moved toward boyhood And fielder Howlin pointed their sniffing unwary eye Where nothing thought souls and he whistled when fast Mad Man Melt Young Willie perched between Dennis Johnson, Thick Soup Throwing Bent Apples, 276 Curtis, Triple-'ones Crown, Manitoba John Hunt at leader\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Temperature** | **Output 1** | **Output 2** | **Output 3** |\n",
        "|-----------------|--------------|--------------|--------------|\n",
        "| 0.3             | Once upon a time in a land far far away, there lived a man named Simeon. He was a wise man, and he knew the secrets of the universe. He knew that the universe was made of many worlds, and that each world was made of a single substance. He knew that each substance was made of a single substance, and that each substance was made | Once upon a time in a land far far away, there lived a man named Simeon. He was a wise man, and he knew the secrets of the universe. He knew that the universe was made of many worlds, and that each world was made of a single substance. He knew that each substance was made of a single substance, and that each substance was made| Once upon a time in a land far far away, there lived a man named Simeon. He was a wise man, and he knew the secrets of the universe. He knew that the universe was made of many worlds, and that each world was made of a single substance. He knew that each substance was made of a single substance, and that each substance was made|\n",
        "| 0.5             | Once upon a time in a land far far away, there was a man who ran a small tavern in a small town, and he had a friend who was in love with a lady in a neighboring town. The friend was a young man, and the friend was a good friend of the man's. The friend had given the young man a very large sum of money, | Once upon a time in a land far far away, a young man was born named Kirito. He was a very shy and timid boy, but he was determined to become a great adventurer. He was raised in a village with a large, beautiful, and beautiful woman named Yukino. When he was young, he was very curious about the world around him |  Once upon a time in a land far far away, there lived a young man named Gavriel. He was tall and handsome, and he had a bright smile. He was also a soldier, and he was on his way to the front lines of the war.Gavriel had always dreamed of becoming a knight. He had been born into a noble family            |\n",
        "| 0.7             | Once upon a time in a land far far away, in a land where stars and planets danced in the nights, a band of heroes became heroes in the land of legend. They are the Pandaur Warriors, the champions of the free world. They are the heroes of the land of legend! |  Once upon a time in a land far far away, a young man crossed the river to live in peace with the natives. As he lived there, he heard of the desire of the people of the forest for a king rather than a king who had been slain by bandits. So he took his journey and came upon a village, near which he saw a large stone dragon.| Once upon a time in a land far far away, a young girl was being experimented with. Her name was Corrine. She had the power to see into the future and could use that power to change the future. The princess, Corrine had been experimented on by Carver, a former high ranking government official. She was given the ability to be an immortal. The |\n",
        "| 0.9             | Once upon a time in a land far far away, Historia Barcelona was not alive, but was in a continuing state. They had a mayor and a mayoress, not to mention a newspaper that was published there, ditto sports books for fashion and celebrity gossip. At a time when pirate radio obsession was a thing, this was the kind of music you could hear.| Once upon a time in a land far far away, a man by the name of Mihi ran a simple game. He gave mud pies to children, and he played each. The children loved children, and played and played until school was out. Then, one day, there was the strong man's poison and the girl's antidote to the poison. The cure was| Once upon a time in a land far far away, a sword knight began his training to become a sword master. The sword knight could augment his own strength at will without using a person as his core for physical training. Able to lift a sword of limitless power and extend it to wherever he desires at will, he could quickly offend even the most vicious mercenaries of all|\n",
        "| 1.1             | Once upon a time in a land far far away, King Jork was trying to bribe and bribe some children, to take plots of land, or heads of great duchies. Well, He got 73 nobility up to said point! But madum da capŝ, It used to rain down before his big gr't mades'. Ice wish to push tools for | Once upon a time in a land far far away, 2 peaceful towns met. Leather rarely exists long enough for a country to spread either, so when the sole white collar resulting Ashton went on another strike formed…..and they both donned… balaclavas! Who knew? | Once upon a time in a land far far away, a king decided giving aquatic living creatures the things of nature was a good idea. This led to everything from lizards giving bats technology and the plaintiffs in our Privacy First lawsuit, the VROO state bordering something little more Mysticism. But quite a few predators of all these vast oceanic wildlife - mainly netted brown bears |"
      ],
      "metadata": {
        "id": "YUd4ikx5JrrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The lower temperatures have much less to no variation, and are repitive. High temperatures have variation but don't make fluent sense, middle temperatures have a mix of both**"
      ],
      "metadata": {
        "id": "0zLR0i1YXoGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Nucleus Sampling (10 pts)\n",
        "\n",
        "Originally published in [Holtzmann et al. (2021)](https://arxiv.org/abs/1904.09751), nucleus sampling was designed to address an issue that was especially prevalent in language models at the time.\n",
        "\n",
        "This issue is the case of \"neural text degeneration,\" where outputs from LMs would often degenerate into gibberish if a low probability token was ever decoded. To address this, nucleus (also known as top-p) sampling uses a hyperparameter, p, to control how big of a subset of the vocabulary we sample from at each step. For example, if p=0.9, we only sample from the subset of tokens that have a cumulative probability mass of 0.9 (after sorting by probability).\n",
        "\n",
        "Implement nucleus sampling and then use the same prompt as above and test your implementation with the following p-values: [0.97, 0.95, 0.9, 0.8, 0.7]\n",
        "What do you notice across outputs?"
      ],
      "metadata": {
        "id": "z-_Y6K1BJwZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vals = [0.97, 0.95, 0.9, 0.8, 0.7]\n",
        "prompt = \"Once upon a time in a land far far away,\"\n",
        "lm = LM()\n",
        "\n",
        "for p in vals:\n",
        "    output = lm.generate(prompt, p=p, temperature=1.0)\n",
        "    print(f\"p={p}\\n{output}\")\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "zEbS7DClQ13W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8769b5da-0ec3-4f82-dd53-f9a8aa4b533a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p=0.97\n",
            "Once upon a time in a land far far away, important messages were communicated to all land. Though extraordinary, these are unheard of in this time. One of these is written in the ancient Book of Legends. This book depicts an event that took place during a rising and town fell; In the sky was growing kindle, trapped between the stars and scorching the land.\n",
            "\n",
            "\n",
            "\n",
            "p=0.95\n",
            "Once upon a time in a land far far away, every living being was turned into a doll, a doll in an otherwise naturalistic manner but utilizing an artificial intelligence with the ability to grow and function naturally again, even if for only a short time.\n",
            "\n",
            "Creation\n",
            "\n",
            "Grady Stacy was created by Kristopher Ryba for Marvel's podcast in 2012, originally\n",
            "\n",
            "\n",
            "\n",
            "p=0.9\n",
            "Once upon a time in a land far far away, upon a soil of glass, there lived a man named Mengele. There was one day when he asked me whether I was a Jew or not. I told him I didn't know, and it surprised him. He replied: \"When you give us money, please don't mention your name. And when we\n",
            "\n",
            "\n",
            "\n",
            "p=0.8\n",
            "Once upon a time in a land far far away, in the same kind of land as this,\" said the old man, looking down at her, \"it happened that a grey haired girl named Elaine slept with a wolf. A wolf which not even a man would dare sleep with.\"\n",
            "\n",
            "\"You haven't got any answers for me,\" muttered Hermione.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "p=0.7\n",
            "Once upon a time in a land far far away, they're shown how to make love in love's kitchen.\n",
            "\n",
            "Get Informed:\n",
            "\n",
            "Name: Gwen Stefani\n",
            "\n",
            "Age: 27\n",
            "\n",
            "Location: New York City\n",
            "\n",
            "Hometown: Pittsburgh, Pennsylvania\n",
            "\n",
            "Votes: 1,102\n",
            "\n",
            "Latest Celebrity Quote: \"I've always\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **p** | **Output 1** | **Output 2** | **Output 3** |\n",
        "|----------|--------------|--------------|--------------|\n",
        "| 0.97     | Once upon a time in a land far far away, we were pulled into a new empire and tried to adapt our military plans to the new culture we were raised in by a spy named Leo. I understand you are a bit reluctant but I will trust you to adapt. I'll keep the word of the man and your spinelessness won't prevent me from deducing the | Once upon a time in a land far far away, when we got to ski or snowboard, many more did the prep than we did. That's because it's the equivalent of one climbing season per person. This was 100 years ago. A soccer cup has added another 40 years to our lives.| Once upon a time in a land far far away, there was an elf who took to politics. Someone who fought against the warlords that lived under it. An elf-writer living in a tavern running a magical journal. But then one day, she felt something her editor wouldn't tell her. She whispered to the manager of her pub, asking them for feedback on her|\n",
        "| 0.95     | Once upon a time in a land far far away, by my fathers name Rawdydom , I was a courier and hard of hearing, too young, Though much or little wistful, I have sat there Many a week, usually droning away,Sometimes more entranced than ever, Refreshing with  | Once upon a time in a land far far away, There came a maiden who said: Your art thou, stranger, with a sly heart? The dueling voice is not of mine, But instead you eluded me, thou demon,I had sworn to put you to death, But I strove not. | Once upon a time in a land far far away, this question was posed to Ah's teacher of law. This mostly just rubbed everyone the wrong way. Over time, each learner's teacher struggled to prove that there wasn't something strange about dealing with a sage, and eventually, a statue of Ah fell from Mt. Tai. Even so, even those who|\n",
        "| 0.9      |Once upon a time in a land far far away, Manaa was about to escape from the greatest threat that Manaa ever encountered. However, she realized that the game had ended. A small incantation appeared over her ear, which made her feel unwell. She was forced to leave in the middle of the night.Her mother, whose business is travelling             | Once upon a time in a land far far away, an evil sorcerer named Bane entered the Far Shores with his evil are alive called of evil. Bane's plan was simple, to regain the power of the Black Prince without having to resort to evil. Having well known the Black Prince's weakness, Bane travelled the world seeking to gain his attention. His first target was Warren| Once upon a time in a land far far away, the Dragon party reached White Rock Camp in the Karak Orphanage. And Yoki attended his orders to kill King Bodo Aalds without question. However the full truth of events transpired, only to set Yoki on a new course of action once he returned to Karak. A king   |\n",
        "| 0.8      | Once upon a time in a land far far away, there lived a man named Erma. She was a bright and pretty girl. She was an amazing singer and dancer. And she loved her goddess. It wasn't much, but it was something. And when the god of thunder, Valamir, came down from the heavens, he was delighted. He asked her| Once upon a time in a land far far away, the ancient king died and his last royal corpse was laid to rest. Many years later, another prince became king. He, too, died and his final royal corpse was laid to rest. Finally, a third prince who ruled over an empire which was split between six kingdoms appeared and seized the throne. The king wished to | Once upon a time in a land far far away, my mind had wander into the wilds of mystery and violence, and I was made to kneel before a great demon. In those days there were no heroes of a moral nature; The only question was which of my old men had the courage to oppose this villain With a sword at his |\n",
        "| 0.7      | Once upon a time in a land far far away, there lived a boy named Teemo. The boy who was like many others, loved spending time with his friends. In the age of Diablo, the village he lived in was besieged by an invading orc. Teemo was so tired of fighting that he decided to rest. This did not go unnoticed by his friends, and| Once upon a time in a land far far away, there lived a strong man, who loved his land very much. He was wise, very kind, and cared very much for his people. And he was a good shepherd. So the man named Simcha lived a long time, and then his land was scattered over the lands of the gods. He fed his people,| Once upon a time in a land far far away, on a distant island, two men in their thirties were fighting. As they were on the verge of killing each other, one of them pulled out a sword and thrust it into the other's chest. The one who lost his arm and lost his life was no longer able to speak. He  |"
      ],
      "metadata": {
        "id": "eWKKAdEeVxBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With higher P values, the outputs use more complex language. Lower p-values sound more like how a child would tell a story, whereas higher p has a more diverse language and structure (such as poem format, which i had to cut out to put into the table here)**"
      ],
      "metadata": {
        "id": "42kgPc4UXsHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 More variations on decoding algorithms (10 pts)\n",
        "\n",
        "Nucleus sampling was definitely not the end of the road in terms of new decoding algorithms. Even in the past few years, new decoding algorithms have been proposed to address some limitations of existing algorithms.\n",
        "\n",
        "Two in particular are:\n",
        "- Typical Sampling ([Meister et al. (2022)](https://arxiv.org/abs/2202.00666))\n",
        "- Eta Sampling ([Hewitt et al. (2022)](https://arxiv.org/abs/2210.15191))\n",
        "\n",
        "For this question, CHOOSE ONE of the two algorithms presented above. Below, please describe in a few sentences what your chosen algorithm does in a novel way and the broad motivation behind it. Along with this description, present 3 sampled outputs for the same prompt as above (you can use one hyperparameter value for all of these).\n",
        "\n"
      ],
      "metadata": {
        "id": "nsIqM7tAQ3U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Typical sampling seeks to see how \"typical\" or expected a token is. It takes the -log of the probaility and normalizes it, and then compares that to the distribution to find how unexpected it is. The controlling variable here decides how far from the distribution it is willing to look, with a higher value meaning more diverse and less typical words/tokens**"
      ],
      "metadata": {
        "id": "gwkhi_7rWZTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time in a land far far away,\"\n",
        "lm = LM()\n",
        "\n",
        "t = 0.9\n",
        "for i in range(3):\n",
        "    output = lm.typical_sampling(prompt, t=t, temperature=1.0)\n",
        "    print(f\" {i}: {output}\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "JSKneYh4V2C9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea42ed0-42a9-493d-c0de-8b49c726ffca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0: Once upon a time in a land far far away, the pursuit of withered dreams, while fame had been a fawning your share among the untutored leaders, there walked a-- native of enchantments places, a wailing wanderer. Woke, and out he lighted on the cemetery, and in that grave lay the body. Alysanli,\n",
            "\n",
            "\n",
            "\n",
            " 1: Once upon a time in a land far far away, surrounded by deep sea, where everywhere there was natural (not always formless) Water, filled the land with Cloaks so thick with dangerous Qi and under the Cloaks were reefs such as a river.\n",
            "\n",
            "The Cloaks comprised of mobile Zithers and their cables held down these fierce true Water Arrows all night\n",
            "\n",
            "\n",
            "\n",
            " 2: Once upon a time in a land far far away, I was teaching exactly that thing everyone is feeding folks on campus. She repointed her pupils, for she knew her class & incautiously speaks recklessly of the august titles of 'reeowned wombs' & 'so popular for whiskering towards a wife and bells' & hisd lexic magisterily\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Decoding Method | **Output 1** | **Output 2** | **Output 3** |\n",
        "|-----------------|--------------|--------------|--------------|\n",
        "| Typical               | Once upon a time in a land far far away, the albino unnaM personhood movement entered a process of self-discovery where it could \"in both shape and color\" voluntarily before becoming a voice, more so than challenges enveloping start-up film states. The movement centred around fountain penist Gabrielle Elliot's casting, which was            | Once upon a time in a land far far away, a very brave woman called Marigold fell and she cried profusely saying, 'The Road has paved to your door'; and she brought me some lady from Algarve which was a witch, and it was full of witchcraft. – Prince Daniel, Auden four quatre formato 1889 If you | Once upon a time in a land far far away, where there was only the world known we call earth and sky, then the leader of mankind found himself confronted with a surprise. He was faced time and again with the question of who he should be. Now is the time to choose. It's an absolute certainty. |"
      ],
      "metadata": {
        "id": "020-CZLlSbTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 BONUS (Up to 15 pts)\n",
        "\n",
        "Can you find a prompt where the continuations do not differ much across multiple sampling strategies, even when we use high temperatures or high p values? (Hint: Think about overfitting)"
      ],
      "metadata": {
        "id": "0iIkmLhfWtEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = [\n",
        "    \"The capital of the United States is\"\n",
        "]\n",
        "\n",
        "lm = LM()\n",
        "temperature = 1\n",
        "p = 0.8\n",
        "\n",
        "for i in prompt:\n",
        "    print(\"Greedy:\\n\")\n",
        "    print(lm.greedy_decoding(i))\n",
        "    print(\"temp:\")\n",
        "    print(lm.temperature_sampling(i, temperature=temperature))\n",
        "    print(\"\\nNucleus:\\n\")\n",
        "    print(lm.nucleus_sampling(i, p=p, temperature=temperature))\n",
        "    print(\"Typical\")\n",
        "    print(lm.typical_sampling(i, t=0.9, temperature=temperature))\n",
        "    print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "sekTD0Qf7SwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outputs:\n",
        "\n",
        "Greedy:\n",
        "The capital of the United States is Washington, D.C.\n",
        "\n",
        "The capital of the United States is Washington, D.C.\n",
        "\n",
        "The capital of the United States is Washington, D.C.\n",
        "\n",
        "The capital of the United States is Washington, D.C.\n",
        "\n",
        "The capital of the United States is Washington, D.\n",
        "\n",
        "-------------------------\n",
        "\n",
        "Temperature:\n",
        "The capital of the United States is the Republican capital, and it's a large NFL town that's worth a lot of money while playing its full role in society, according to the research company of University College. Ohio has no equal financial passion, argues William Bryson, the lead author of the study, who explains Americans choose their national capital city on \"\n",
        "\n",
        "-------------------------\n",
        "\n",
        "Nucleus:\n",
        "The capital of the United States is Virginia. Virginia has the highest rate of births in the nation. A child born in Virginia born today will have an average of 12.3 years of life in the US.\n",
        "\n",
        "------------\n",
        "Typical:\n",
        "The capital of the United States is Washington, which is difficult to describe. There is a cemetery for the dead situated in the suburbs of Carlsbad, the only town outside the San Ysidro Free Trade Zone with its own government department. It is fascinating to think that the magnificent Welsh landscape was created by Wright Brothers with hydraulic plates that were submerged in\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JjcuUD5UYTY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------\n",
        "**We can see here that the results are pretty similar across each sampling type, althought not exactly. DC borders Virginia and a lot of government buildings are in Virginia, and Republicans do hold a large amount of power in DC, althought it definitely isnt the exact correct answer**"
      ],
      "metadata": {
        "id": "0JUf7_g79U1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Prompting (50 pts)\n",
        "\n",
        "In this problem, we will try various prompting approaches and prompt an LLM for a Math Reasoning Benchmark called [GSM8K](https://github.com/openai/grade-school-math), which contains grade school math word problems. This is a very common _reasoning_ benchmark used to test various LLMs.\n",
        "\n",
        "The LLM that we will be using is [Google Gemini](https://gemini.google.com/). We will be prompting Gemini by using an API call to the Gemini Model. Normally, you can also prompt Open Source LLMs via the HuggingFace Library, however due to compute constraints, we use Gemini in this problem."
      ],
      "metadata": {
        "id": "kweZjWF3LpfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the GSM8K Dataset and Google Gemini\n",
        "\n",
        "Follow the steps below to download the GSM8K Dataset and to setup Google Gemini on Colab. You will automatically get points for this subpr"
      ],
      "metadata": {
        "id": "56o8lVgVNVXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"gsm8k\", 'main')"
      ],
      "metadata": {
        "id": "Cg_sNCLeL5j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset['train']), len(dataset['test'])"
      ],
      "metadata": {
        "id": "fcI6_53zN5if",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f66e18b-0f26-4f7c-a2f4-63aa8ff26bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7473, 1319)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An example instance of this dataset\n",
        "\n",
        "dataset['test'][6]"
      ],
      "metadata": {
        "id": "0zfGgqwZN9EN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e49c568f-3b56-4d2d-8d79-6350f0c68bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How many sheep do Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep?',\n",
              " 'answer': 'If Seattle has 20 sheep, Charleston has 4 * 20 sheep = <<20*4=80>>80 sheep\\nToulouse has twice as many sheep as Charleston, which is 2 * 80 sheep = <<2*80=160>>160 sheep\\nTogether, the three has 20 sheep + 160 sheep + 80 sheep = <<20+160+80=260>>260 sheep\\n#### 260'}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gemini Setup (from the official [Gemini documentation](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/get-started/python.ipynb))\n",
        "\n",
        "\n",
        "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
        "\n",
        "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>\n",
        "\n",
        "In Colab, add the key to the secrets manager under the \"🔑\" in the left panel.\n",
        "\n",
        "---\n",
        "\n",
        "Give it the name `GEMINI_API_KEY`.\n",
        "\n",
        "Once you have the API key, pass it to the SDK. You can do this in two ways:\n",
        "\n",
        "* Put the key in the `GEMINI_API_KEY` environment variable (the SDK will automatically pick it up from there).\n",
        "* Pass the key to `genai.configure(api_key=...)`"
      ],
      "metadata": {
        "id": "CY-vwQI5OmPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports for this question\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from datasets import Dataset\n",
        "import random\n",
        "from typing import Callable, List, Any"
      ],
      "metadata": {
        "id": "QlcHWlpQOIh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "YZOA3PVIPWxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test if your setup is working, do not change the model name\n",
        "model = genai.GenerativeModel(\"gemini-1.0-pro\")\n",
        "response = model.generate_content(\"What is Natural Language Processing? Explain it to a five year old.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "Cv8B6I2lPiMw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "c5daed4a-f365-4675-e7a8-0754c6f507a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you have a super smart computer friend named NLP. NLP can understand and talk to you in the same way you talk to your mom or dad.\n",
            "\n",
            "Just like you learn new words and how to put them together to say sentences, NLP learns how to understand human language. When you say things to NLP, it listens carefully and tries to figure out what you mean.\n",
            "\n",
            "Then, NLP can answer you back in a way that makes sense to you. It's like having a friend who speaks your language perfectly! That's what Natural Language Processing is all about. It's like giving computers the power to understand and talk to us in our own language.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data and Prompting Setup (15 + 5 pts)\n",
        "\n",
        "In this part, we will create some boilerplate code to process our dataset and generate prompts from the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "DAAyTvXeQ5jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the GSM8K Dataset"
      ],
      "metadata": {
        "id": "4njT8xp2apCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_gsm8k_answers(dataset: Dataset) -> Dataset:\n",
        "    \"\"\"\n",
        "    Processes the GSM8K dataset to remove reasoning chains and retain only the numerical answers.\n",
        "    Assumes answers are separated from reasoning by the '###' string.\n",
        "\n",
        "    Args:\n",
        "    dataset (Dataset): Huggingface Dataset object for GSM8K.\n",
        "\n",
        "    Returns:\n",
        "    Dataset: Processed Dataset object with numerical answers only.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_answer(sample):\n",
        "        # IMPLEMENT HERE\n",
        "        # Split the answer using '###' and return a dictionary with the key 'processed_answer'\n",
        "        if '###' in sample['answer']:\n",
        "            processed_answer = sample['answer'].split('###')[-1].strip()\n",
        "        else:\n",
        "            processed_answer = sample['answer'].strip()\n",
        "        return {'processed_answer': processed_answer}\n",
        "\n",
        "    return dataset.map(extract_answer)"
      ],
      "metadata": {
        "id": "6IDGGocfQldW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "\n",
        "gsm8k_test_processed = process_gsm8k_answers(dataset['test'])\n",
        "print(\"Processed Test Dataset Example:\")\n",
        "print(gsm8k_test_processed[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx9JVsV1ZC8V",
        "outputId": "ce6dd8b2-c38c-4aab-fa51-866b510bc7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Test Dataset Example:\n",
            "{'question': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\", 'answer': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18', 'processed_answer': '# 18'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Prompts (15 pts)\n",
        "\n",
        "We will be implementing FIVE (5) prompting methods. See their descriptions below -\n",
        "1. **Zero-Shot Answer Only (2 pts)**: You prompt the model to only generate the answer to the question\n",
        "\n",
        "2. **Zero-Shot Chain of Thought (CoT) (3 pts)**: Refer to the [Chain of Thought Paper](https://arxiv.org/abs/2201.11903). CoT refers to a reasoning chain that is generated by the model before generating the actual answer. This has shown to improve performance. In this setup, you will prompt the model to generate a reasoning chain before the answer.\n",
        "\n",
        "3. **5-Shot Answer Only (2 pts)**: You provide some in-context examples to prompt the model with to generate the answer. This is analogous to Approach 1. Use the a random set of 5 examples from the training set to create the in-context examples.\n",
        "\n",
        "4. **5-Shot CoT (3 pts)**: Combine Approaches 2 and 3 to do 5-shot CoT prompting.\n",
        "\n",
        "5. **Your own prompt! (5 pts)**: Try something new. Think about how you solve Math problems and implement your own prompting method."
      ],
      "metadata": {
        "id": "9ddnQ6b4azH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generation_zero_shot(problem: str) -> str:\n",
        "    \"\"\"\n",
        "    Zero-shot prompt.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    return f\"answer this math problem, I just want only the answer: \\n\\n Problem: {problem}\\nAnswer:\"\n"
      ],
      "metadata": {
        "id": "MiW-lHk1aniq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generation_zero_shot_cot(problem: str) -> str:\n",
        "    \"\"\"\n",
        "    Zero-shot Chain of Thought (CoT) prompt.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    return f\"answer the following math problem. First, think through the steps logically and then give an answer:\\n\\nProblem: {problem}\\n\\n Explanation and answer:\""
      ],
      "metadata": {
        "id": "8gSts3jebOLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generation_5_shot(problem: str, training_set: Dataset) -> str:\n",
        "    \"\"\"\n",
        "    5-shot prompt generation for GSM8K problems. Randomly selects 5 examples from the training set.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt with 5 in-context_examples.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "\n",
        "    examples = random.sample(list(training_set), 5)\n",
        "    examps = \"\\n\\n\".join(\n",
        "        [f\"Problem: {ex['question']}\\nAnswer: {ex['processed_answer']}\" for ex in examples]\n",
        "    )\n",
        "    return f\"{examps}\\n\\n Problem: {problem}\\nAnswer:\"\n"
      ],
      "metadata": {
        "id": "G1RQZKU-bbi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generation_5_shot_wrapper(problem: str) -> str:\n",
        "    t_set = process_gsm8k_answers(dataset['train'])\n",
        "    return prompt_generation_5_shot(problem, t_set)"
      ],
      "metadata": {
        "id": "of0sh9A_gSxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generation_5_shot_cot(problem: str, training_set: Dataset) -> str:\n",
        "    \"\"\"\n",
        "    5-shot Chain of Thought (CoT) prompt generation. Randomly selects 5 examples\n",
        "    from the training set and includes reasoning steps.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt with 5 CoT in-context examples.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    examples = random.sample(training_set, 5)\n",
        "    examps = \"\\n\\n\".join(\n",
        "        [f\"Problem: {ex['question']}\\nReasoning: {ex['answer']}\\nFinal Answer: {ex['processed_answer']}\" for ex in examples]\n",
        "    )\n",
        "\n",
        "    return f\"{examps}\\n\\n Problem: {problem}\\n Reasoning and answer:\""
      ],
      "metadata": {
        "id": "UNteVGTWbnLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generation_5_shot_cot_wrapper(problem: str) -> str:\n",
        "    t_set = list(process_gsm8k_answers(dataset['train']))\n",
        "    return prompt_generation_5_shot_cot(problem, t_set)"
      ],
      "metadata": {
        "id": "vgYR_sXViZbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change the method definition\n",
        "\n",
        "def my_prompt(problem: str) -> str:\n",
        "    \"\"\"\n",
        "    Your own unique way of prompting an LLM for Math word problems.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    return f\"solve the following math problem step by step. once you reach the answer, verify if its correct by checking your solution.\\n\\nProblem: {problem}\\nSteps:\\nAnswer:\""
      ],
      "metadata": {
        "id": "8OhEcDetbP3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Prompting Gemini and Implementing Self-Consistency (5 + 5 + 10 pts)\n",
        "\n",
        "Here, you will help build the wrapper for prompting Gemini using the prompt methods you have designed above.\n",
        "\n",
        "You will then also implement Self-Consistency based prompting. Refer to the [Self-Consistency Paper](https://arxiv.org/abs/2203.11171). In order to implement Self-Consistency, you generate multiple Zero-Shot CoT (Approach 2 in the prompting methods) candidates, and take a majority vote of the answers predicted by each candidate."
      ],
      "metadata": {
        "id": "j4uARh18cXDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First, write the function where you will process the answer generated by the model. (5 pts)\n",
        "\n",
        "Note that answer processing changes for different prompt types, so this function also takes in the name of the method in its argument."
      ],
      "metadata": {
        "id": "XuVhq91Oc5TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def answer_processing(prediction: str, prompt_function: Any) -> str:\n",
        "    \"\"\"\n",
        "    Processes the model's generated output to extract the final answer.\n",
        "\n",
        "    Returns:\n",
        "    str: The processed numerical answer.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_name = prompt_function.__name__\n",
        "\n",
        "    answer = None\n",
        "    if \"Answer:\" in prediction:\n",
        "        answer = prediction.split(\"Answer:\")[-1].strip()\n",
        "    elif \"The answer is\" in prediction:\n",
        "        answer = prediction.split(\"The answer is\")[-1].strip()\n",
        "    else:\n",
        "        answer = prediction.strip().split('\\n')[-1].strip()\n",
        "\n",
        "    num = re.findall(r\"\\d+\", answer)\n",
        "    if num:\n",
        "        return f\"# {''.join(num)}\"\n",
        "    else:\n",
        "        return \"# 0\""
      ],
      "metadata": {
        "id": "TDZwuVpeUCyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change, method to calculate accuracy from predictions and ground truth labels\n",
        "\n",
        "def evaluate_accuracy(predictions: List[str], ground_truths: List[str]) -> float:\n",
        "    correct = 0\n",
        "    total = len(predictions)\n",
        "\n",
        "    for pred, true in zip(predictions, ground_truths):\n",
        "        if pred == true:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy * 100"
      ],
      "metadata": {
        "id": "hWOHGuWzUF2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next, write the wrapper function where you use all the building blocks constructed above to prompt the Gemini model (5 + 10 pts)\n",
        "\n",
        "\n",
        "On how to prompt Gemini, refer to the [Gemini Text Generation Handbook](https://ai.google.dev/gemini-api/docs/text-generation?lang=python).\n",
        "\n",
        "Hint: Reading this will help you figure out how to generate multiple candidates to implement Self-Consistency."
      ],
      "metadata": {
        "id": "cbXg7blmfZeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_generate(\n",
        "    model_instance: Any,\n",
        "    test_set: Dataset,\n",
        "    prompt_function: Callable[[str], str],\n",
        "    process_answer_function: Callable[[str], str],\n",
        "    evaluation_function: Callable[[List[str], List[str]], float],\n",
        "    self_consistency: int,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    model_instance (Any): The Google Gemini model instance.\n",
        "    test_set (Dataset): The GSM8K test set to evaluate on.\n",
        "    prompt_function (Callable): Function to generate prompts for the test set.\n",
        "    process_answer_function (Callable): Function to process the model's generated answers.\n",
        "    evaluation_function (Callable): Function to evaluate model's answers against the ground truth.\n",
        "    self_consistency: Number of samples to run self-consistency approach on.\n",
        "    If negative, 0 or 1, this implies regular prompting\n",
        "\n",
        "    Returns:\n",
        "    float: The accuracy of the model on the test set.\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = []\n",
        "    grounds = []\n",
        "\n",
        "    for sample in test_set:\n",
        "        problem = sample[\"question\"]\n",
        "        ground = sample[\"processed_answer\"]\n",
        "\n",
        "        #print(f\"Problem: {problem}\")\n",
        "        #print(f\"ground answer: {ground}\")\n",
        "\n",
        "        if self_consistency > 1:\n",
        "            answers = []\n",
        "            for _ in range(self_consistency):\n",
        "                prompt = prompt_function(problem)\n",
        "                #print(f\"\\n{prompt}\")\n",
        "                response = model_instance.generate_content(prompt)\n",
        "                #print(f\"output:\\n{response.text}\")\n",
        "                processed_answer = process_answer_function(response.text, prompt_function)\n",
        "                #print(f\"answer: {processed_answer}\")\n",
        "                answers.append(processed_answer)\n",
        "\n",
        "            final_answer = max(set(answers), key=answers.count)\n",
        "        else:\n",
        "            prompt = prompt_function(problem)\n",
        "            #print(f\"\\n{prompt}\")\n",
        "            response = model_instance.generate_content(prompt)\n",
        "            #print(f\"output:\\n{response.text}\")\n",
        "            final_answer = process_answer_function(response.text, prompt_function)\n",
        "            #print(f\"final answer: {final_answer}\")\n",
        "\n",
        "        predictions.append(final_answer)\n",
        "        grounds.append(ground)\n",
        "\n",
        "    accuracy = evaluation_function(predictions, grounds)\n",
        "    #print(f\"Accuracy: {accuracy}%\")\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "Y0JOy65dTNJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = list(process_gsm8k_answers(dataset['train']))\n",
        "print(\"Sample from the training set:\", training_set[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6RksucmkDDt",
        "outputId": "1d7f96f7-5290-4325-b91a-6abb6b23ce14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample from the training set: {'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72', 'processed_answer': '# 72'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_test_processed = process_gsm8k_answers(dataset['test'])\n",
        "\n",
        "# The following line is just to test your systems, comment this line out to report results on the entire test set in 3.3\n",
        "gsm8k_test_processed = Dataset.from_dict(gsm8k_test_processed[:5])\n",
        "\n",
        "# Run model generation with zero-shot prompt generation\n",
        "accuracy = pipeline_generate(\n",
        "    model_instance=model,\n",
        "    test_set=gsm8k_test_processed,\n",
        "    prompt_function=prompt_generation_zero_shot_cot, # Change this to test different prompt methods\n",
        "    process_answer_function=answer_processing,\n",
        "    evaluation_function=evaluate_accuracy,\n",
        "    self_consistency=5,\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "id": "tzHoeMnyUIYp",
        "outputId": "d656a584-a1ca-4cd7-c0bd-3ce685e9af8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Complete this table based on your implementation in 3.2 and answer the following questions (5 + 5 pts)"
      ],
      "metadata": {
        "id": "NPt0dz1lgpZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Round each value up to two decimal points (5 pts)\n",
        "\n",
        "Method|Accuracy\n",
        "---|---|\n",
        "0-shot| 80.00%\n",
        "0-shot CoT| 80.00%\n",
        "5-shot| 60.00%\n",
        "5-shot CoT| 80.00%\n",
        "My prompt| 20.00%\n",
        "0-shot CoT Self-Consistency| 60.00%"
      ],
      "metadata": {
        "id": "3rSganJ6gz8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What was the intuition behind the prompt that you designed? (2 pts)\n",
        "\n",
        "**The intuition behind it is that it forces chain of thought reasoning, where the model has to provide logical reasoning for each step of it's decisions. This, along with the verification request at the end, helps assure the process is logical and more accurate.**"
      ],
      "metadata": {
        "id": "HvW7VTADhLa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are the merits and demerits of using advanced prompting approaches like Chain of Thought or Self-Consistency? (3 pts)\n",
        "\n",
        "*   **Merits: More understanable to a user, rather than just giving an answer it gives intuition which justifies each step. This also improves accuracy, since the model can't hallucinate as much, it must have logic. This is how humans solve math problems so it correlates more to human use. Self consistency is able to find the most commonly appearing result, meaning it results from many possible approaches so its a strong candidate.**\n",
        "  \n",
        "*   **Demerits: For easy tasks, its a waste of resource and uses an unneccessary amount of computation. Self consistency can pose an issue if the model keeps messing up computations, skewing the most common results toward incorrect ones.**"
      ],
      "metadata": {
        "id": "oSknWcWrhedg"
      }
    }
  ]
}