{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1. n-Gram Language Models (30 points)\n",
        "\n",
        "Create 1-5-gram language models trained on the tiny shakespeare dataset. The relevant probabilities P(word|context) will be stored in Python dictionaries. Do not use any smoothing or back-off (until Question 4). Pay special attention to beginning and end of sequences in the modeling process.\n",
        "\n",
        "In this section, you may only use libraries imported in the original template."
      ],
      "metadata": {
        "id": "lOHLpU5y0Wgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libaries\n",
        "import requests\n",
        "import collections\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "KQ0G021VWiOh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "response.raise_for_status() # Raise an exception for invalid HTTP status codes\n",
        "text_data = response.text\n",
        "len(text_data), text_data[:100]"
      ],
      "metadata": {
        "id": "Ya2CfD3vWk2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f737aeb2-4d44-4e36-c168-797507920d19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115394,\n",
              " 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample\n",
        "random.seed(42)\n",
        "\n",
        "pos = random.randint(0, len(text_data) - 1000)\n",
        "print(text_data[pos:pos+100])"
      ],
      "metadata": {
        "id": "HtJfs33cWmBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860cac0e-2c9d-47c8-d1a7-abde2b6a5622"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BY:\n",
            "Many good morrows to my noble lord!\n",
            "\n",
            "HASTINGS:\n",
            "Good morrow, Catesby; you are early stirring\n",
            "What\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing - do not change\n",
        "def preprocess_text(text_data):\n",
        "  text_data = text_data.replace(',',' , ').replace(';', ' ').replace(':', ' ').replace('.',' . ').replace('?',' ? ').replace('!',' ! ')\n",
        "  text_data = text_data.replace('-', ' ')\n",
        "  text_data = text_data.replace('\\'', '').replace('\"', '')\n",
        "  text_data = text_data.replace('  ', ' ')\n",
        "  text_data = text_data.replace('\\n\\n','\\n').replace('\\n',' </s> <s> ')\n",
        "  text_data = '<s> ' + text_data + ' </s>'\n",
        "  text_data = text_data.lower()\n",
        "  return text_data\n",
        "\n",
        "text_data = preprocess_text(response.text)\n",
        "print(f\"Number of words: {len(text_data.split(' '))}\")"
      ],
      "metadata": {
        "id": "SaPIqFwHWpA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5c065b-063e-4bca-c0c4-a3daa90aefbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words: 328097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = text_data[:-10_000]\n",
        "test_data = text_data[-10_000:]\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "cnHQ-enjWqVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd8182c2-09ae-4a89-dd96-a340d07f5baa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1431030, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(train_data.split(' '))\n",
        "print(f\"Number of unique words: {len(vocab)}\")\n",
        "print(f\"Sample unique words: {list(vocab)[:10]}\")"
      ],
      "metadata": {
        "id": "IrYnPsMKWrvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf5169e1-0332-4ee6-8c53-0bdc32be605e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 12124\n",
            "Sample unique words: ['', 'instantly', 'coral', 'sweet', 'fair', 'tames', 'disprove', 'unconstant', 'disbursed', 'griping']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## A. Dealing with Out of Vocabulary Words  (5 Points)\n",
        "\n",
        "We want a way to handle words that did not appear in the train set. Create a list of out of vocabulary words by identifying words that appear in the train dataset less than 3 times.\n",
        "\n",
        "**Hint:** You can split the dataset into terms by using corpus.split(' '). You can assume the results will be valid word tokens. No more preprocessing is necessary."
      ],
      "metadata": {
        "id": "9YO9DwrkWuQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_oov_words(corpus, n=3):\n",
        "    \"\"\"\n",
        "    Identify out-of-vocabulary (OOV) words that appear less than `n` times in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: The dataset to process. It should be a dictionary with a 'text' key.\n",
        "    - n: The frequency threshold below which words are considered OOV.\n",
        "\n",
        "    Returns:\n",
        "    - A set of out-of-vocabulary words.\n",
        "    \"\"\"\n",
        "    # INSERT CODE HERE\n",
        "\n",
        "    words = corpus.split(' ')\n",
        "    word_counts = collections.Counter(words)\n",
        "    oov_words = {word for word, count in word_counts.items() if count < n}\n",
        "    return oov_words\n",
        "\n"
      ],
      "metadata": {
        "id": "CzuEDZoZWs7t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_words = identify_oov_words(train_data)\n",
        "\n",
        "vocab = vocab - oov_words\n",
        "vocab.add('<UNK>')\n",
        "print(f\"Number of OOV words: {len(oov_words)}\")\n",
        "print(f\"Expected number of OOV words: {7181}\")\n",
        "\n",
        "assert len(oov_words) == 7181"
      ],
      "metadata": {
        "id": "S3_q0smRW2Ex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbcecb02-905f-4c63-f745-313a0862311e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of OOV words: 7181\n",
            "Expected number of OOV words: 7181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ' '.join(['<UNK>' if word not in vocab else word for word in train_data.split(' ')])\n",
        "test_data = ' '.join(['<UNK>' if word not in vocab else word for word in test_data.split(' ')])"
      ],
      "metadata": {
        "id": "L60G-KIEW5KA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Create the N-Gram Models (12 Points)\n",
        "In this section, we will train several N-Gram models on ONLY the train_data.\n",
        "\n",
        "First, calculate the counts of each N-Gram."
      ],
      "metadata": {
        "id": "LHKNJJ-gW6qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uni_counts = collections.defaultdict(lambda:0)\n",
        "bi_counts = collections.defaultdict(lambda:0)\n",
        "tri_counts = collections.defaultdict(lambda:0)\n",
        "four_counts = collections.defaultdict(lambda:0)\n",
        "five_counts = collections.defaultdict(lambda:0)"
      ],
      "metadata": {
        "id": "s6p1troiW9lF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = train_data.split(' ')\n",
        "\n",
        "for i in range(len(words)):\n",
        "    uni_counts[words[i]] += 1\n",
        "    if i < len(words) - 1:\n",
        "        bi_counts[(words[i], words[i + 1])] += 1\n",
        "    if i < len(words) - 2:\n",
        "        tri_counts[(words[i], words[i + 1], words[i + 2])] += 1\n",
        "    if i < len(words) - 3:\n",
        "        four_counts[(words[i], words[i + 1], words[i + 2], words[i + 3])] += 1\n",
        "    if i < len(words) - 4:\n",
        "        five_counts[(words[i], words[i + 1], words[i + 2], words[i + 3], words[i + 4])] += 1\n"
      ],
      "metadata": {
        "id": "nY3UMFE6W_ys"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, implement the N-Gram models themselves."
      ],
      "metadata": {
        "id": "7DdgHEI5XB0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uni = collections.defaultdict(lambda:0)\n",
        "bi = collections.defaultdict(lambda:0)\n",
        "tri = collections.defaultdict(lambda:0)\n",
        "four = collections.defaultdict(lambda:0)\n",
        "five = collections.defaultdict(lambda:0)"
      ],
      "metadata": {
        "id": "-aACGp05XAVW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = sum(uni_counts.values())\n",
        "\n",
        "for word, count in uni_counts.items():\n",
        "    uni[word] = count / total_words\n",
        "for (w1, w2), count in bi_counts.items():\n",
        "    bi[(w1, w2)] = count / uni_counts[w1]\n",
        "for (w1, w2, w3), count in tri_counts.items():\n",
        "    tri[(w1, w2, w3)] = count / bi_counts[(w1, w2)]\n",
        "\n",
        "for (w1, w2, w3, w4), count in four_counts.items():\n",
        "    four[(w1, w2, w3, w4)] = count / tri_counts[(w1, w2, w3)]\n",
        "\n",
        "for (w1, w2, w3, w4, w5), count in five_counts.items():\n",
        "    five[(w1, w2, w3, w4, w5)] = count / four_counts[(w1, w2, w3, w4)]\n"
      ],
      "metadata": {
        "id": "CBnpHlliY3wP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "assert five[('<s>', 'against', 'the', 'roman', 'state')] == 1.0 # prob of last given prev 4\n",
        "assert four[('remain', '</s>', '<s>', 'i')] == 0.25 # prob of last given prev 3\n",
        "assert tri[('did', 'see', 'and')] == 0.5 # prob of last given prev 2\n",
        "assert bi[('rash', 'like')] == 0.1 # prob of last given prev 1\n",
        "assert round(uni[('citizen')],5) == 0.00031 # prob of last"
      ],
      "metadata": {
        "id": "CUgTaWzVXDsQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Laplace Smoothing (6 Points)\n",
        "\n",
        "From now on, we will **focus our attention to the bigram model**. Reimplement the bigram model using Laplacian Smoothing."
      ],
      "metadata": {
        "id": "Le7l1pSAXGWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_bigram_probability_with_smoothing(word1, word2):\n",
        "\n",
        "    bigram_count = bi_counts[(word1, word2)] + 1\n",
        "    unigram_count = uni_counts[word1] + len(vocab)\n",
        "    return bigram_count / unigram_count"
      ],
      "metadata": {
        "id": "sVXNg0bZXLfM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Evaluate Perplexity (7 Points)\n",
        "\n",
        "Now let's evaluate the smoothed bigram model quantitively using the intrinsic metric **perplexity**.\n",
        "\n",
        "Recall perplexity is the inverse probability of the test text\n",
        "$$\\text{ppl}(w_1, \\dots, w_n) = p(w_1, \\dots, w_n)^{-\\frac{1}{N}}$$\n",
        "\n",
        "For an n-gram model, perplexity is computed by\n",
        "$$\\text{ppl}(w_1, \\dots, w_n) = (\\prod_i p(w_{i+n}|w_i^{i+n-1})^{-\\frac{1}{N}}$$\n",
        "\n",
        "To get rid of numerical issue, we usually compute through:\n",
        "$$\\text{ppl}(w_1, \\dots, w_n) = \\exp(-\\frac{1}{N}\\sum_i \\log p(w_{i+n}|w_i^{i+n-1}))$$\n",
        "\n",
        "Note that you do NOT need to optimize the language model in any way so as to minimize perplexity. Your reported perplexity will have no correlation with your score on this assignment, as far as it is implemented correctly.\n",
        "\n",
        "**HINT:** Use the calculate_bigram_probability_with_smoothing function created above."
      ],
      "metadata": {
        "id": "p70s9JZcXQIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_perplexity(data):\n",
        "  \"\"\"\n",
        "    Computes the perplexity of a given text data using a bigram language model.\n",
        "\n",
        "    Parameters:\n",
        "    - data : str\n",
        "    Returns:\n",
        "    - float\n",
        "  \"\"\"\n",
        "\n",
        "  assert len(data.split(' ')) >= 5\n",
        "  # Hint: You should use the math library for exp and log\n",
        "  # INSERT CODE HERE\n",
        "\n",
        "  words = data.split(' ')\n",
        "\n",
        "  log_sum = 0\n",
        "\n",
        "  for i in range(len(words) - 1):\n",
        "      word1 = words[i]\n",
        "      word2 = words[i + 1]\n",
        "      prob = calculate_bigram_probability_with_smoothing(word1, word2)\n",
        "      log_sum += math.log(prob)\n",
        "\n",
        "  perplexity = math.exp(-log_sum / (len(words)))\n",
        "\n",
        "  print(f\"perplexity: {perplexity}\")\n",
        "\n",
        "  return perplexity\n"
      ],
      "metadata": {
        "id": "pqODu8L-XPS9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert round(compute_perplexity(test_data)) == 129"
      ],
      "metadata": {
        "id": "vVLFr2EfXWOH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e688ccfe-a2b7-4555-bc67-ec3bf445699d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity: 129.43406024742046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. Logistic Regression (40 points)\n",
        "\n"
      ],
      "metadata": {
        "id": "c-evK15Gwf3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this question, you will be guided to implement logistic regression classifer from scratch. You will use LR classifer to do sentiment analysis task on Twitter dataset (the dataset is provided in the code)."
      ],
      "metadata": {
        "id": "1iiTuJWnfhA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "M7etuJ5pwymq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to import nltk\n",
        "import nltk"
      ],
      "metadata": {
        "id": "Tw63O_jqwfJ1"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "# you only need to run it once\n"
      ],
      "metadata": {
        "id": "HotoYOViw3Bv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884cdb22-35d4-4b8f-f92f-7922a95e6f3c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "O8KazUgww7rU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data\n",
        "* You do not need to split the data. We provide the code for you and you just need to run the code below.\n"
      ],
      "metadata": {
        "id": "0zmbOD3-xgpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "lq8Rr5R4xc8d"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train test split: 20% will be in the test set, and 80% in the training set.\n",
        "\n"
      ],
      "metadata": {
        "id": "pd-qM1SBxlQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ],
      "metadata": {
        "id": "SJsYMwnoxodO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create the numpy array of positive labels and negative labels."
      ],
      "metadata": {
        "id": "usI6KCj6xtBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "CVEhgGkix0UR"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.  Text processing (6 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Remove old style retweet with 'RT' in the sentence\n",
        "*   Remove hyperlinks\n",
        "\n",
        "*   Remove hashtag\n",
        "*   Tokenize the sentence using TweetTokenizer\n",
        "\n",
        "\n",
        "*   Remove stop words\n",
        "*   Use PorterStemmer to create stem of words in tweet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vqd8BjfkxDaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweet(tweet):\n",
        "\n",
        "  stemmer = PorterStemmer()\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  processed_tweet = []\n",
        "  for word in tweet_tokens:\n",
        "      if (word not in stop_words and\n",
        "          word not in string.punctuation):\n",
        "          stemmed_word = stemmer.stem(word)\n",
        "          processed_tweet.append(stemmed_word)\n",
        "\n",
        "  return processed_tweet"
      ],
      "metadata": {
        "id": "Cp6zvJ3uw-rP"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a function that will take tweets and their labels as input, go through every tweet, preprocess them, count the occurrence of every word in the data set and create a frequency dictionary.\n",
        "\n",
        "Notice how the outer for loop goes through each tweet, and the inner for loop steps through each word in a tweet.\n",
        "The freqs dictionary is the frequency dictionary that's being built.\n",
        "The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0). The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
      ],
      "metadata": {
        "id": "SeW_hTjhxTQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "\n",
        "    freqs = {}\n",
        "\n",
        "    for tweet, label in zip(tweets, ys):\n",
        "        processed_tweet = process_tweet(tweet)\n",
        "\n",
        "        label = label.item()\n",
        "\n",
        "        for word in processed_tweet:\n",
        "            pair = (word, label)\n",
        "\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "    return freqs"
      ],
      "metadata": {
        "id": "L-s8dg8NxX4i"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Logistic regression (16 points)\n",
        "\n",
        "\n",
        "### Sigmoid (4 points)\n",
        "\n",
        "You will learn to use logistic regression for text classification.\n",
        "* The sigmoid function is defined as:\n",
        "\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
        "\n",
        "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability.\n"
      ],
      "metadata": {
        "id": "A5iTSkQHx6I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sigmoid(z):\n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "\n",
        "    # write your code here\n",
        "    z = np.clip(z, -100, 100) #divide by zero error\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    return h"
      ],
      "metadata": {
        "id": "WP5U6cRDyFqb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*If you are familiar with Logistic regression, you don't need to go over this instructions. *\n",
        "\n",
        "### Logistic regression: regression and a sigmoid\n",
        "\n",
        "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
        "\n",
        "Regression:\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "Note that the $\\theta$ values are \"weights\". If you took the Deep Learning Specialization, we referred to the weights with the `w` vector.  In this course, we're using a different variable $\\theta$ to refer to the weights.\n",
        "\n",
        "Logistic regression\n",
        "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
        "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "We will refer to 'z' as the 'logits'.\n",
        "\n",
        "###  Cost function and Gradient\n",
        "\n",
        "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
        "* $m$ is the number of training examples\n",
        "* $y^{(i)}$ is the actual label of the i-th training example.\n",
        "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
        "\n",
        "The loss function for a single training example is\n",
        "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
        "\n",
        "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
        "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0.\n",
        "\n",
        "### Update the weights\n",
        "\n",
        "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
        "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
        "\n",
        "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n",
        "* 'i' is the index across all 'm' training examples.\n",
        "* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n",
        "\n",
        "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
        "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
        "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
      ],
      "metadata": {
        "id": "Ckn5acahyKHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement gradient descent function (12 points)\n",
        "\n",
        "* The number of iterations `num_iters` is the number of times that you'll use the entire training set.\n",
        "* For each iteration, you'll calculate the cost function using all training examples (there are `m` training examples), and for all features.\n",
        "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
        "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
        "\\theta_0\n",
        "\\\\\n",
        "\\theta_1\n",
        "\\\\\n",
        "\\theta_2\n",
        "\\\\\n",
        "\\vdots\n",
        "\\\\\n",
        "\\theta_n\n",
        "\\end{pmatrix}$$\n",
        "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
        "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
        "    * $\\mathbf{x}$ has dimensions (m, n+1)\n",
        "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
        "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
        "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
        "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
        "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
        "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
        "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
      ],
      "metadata": {
        "id": "VYaPWAXoy8E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "    '''\n",
        "    ### Write your code here\n",
        "\n",
        "    m = len(y)\n",
        "\n",
        "    J_cost = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        z = np.dot(x, theta)\n",
        "        h = sigmoid(z)\n",
        "        h = np.clip(h, 1e-10, (1 - 1e-10))\n",
        "\n",
        "        J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
        "        J_cost.append(J[0][0])\n",
        "\n",
        "        #grad = (alpha/m) *\n",
        "\n",
        "        theta = theta - ((alpha/m) * np.dot(x.T, (h - y)))\n",
        "\n",
        "        #  print(f\"cost: {J[0][0]}\")\n",
        "\n",
        "    return J_cost, theta\n",
        "\n"
      ],
      "metadata": {
        "id": "u5ylWyKIzEqs"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Extracting the features (6 points)\n",
        "\n",
        "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
        "    * The first feature is the number of positive words in a tweet.\n",
        "    * The second feature is the number of negative words in a tweet.\n",
        "* Then train your logistic regression classifier on these features.\n",
        "* Test the classifier on a validation set.\n",
        "\n",
        "### Instructions: Implement the extract_features function.\n",
        "* This function takes in a single tweet.\n",
        "* Process the tweet using the imported `process_tweet()` function and save the list of tweet words.\n",
        "* Loop through each word in the list of processed words\n",
        "    * For each word, check the `freqs` dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n",
        "    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)"
      ],
      "metadata": {
        "id": "kOW5hhOezOnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3))\n",
        "\n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1\n",
        "\n",
        "    # write your code here\n",
        "    pos_words = 0\n",
        "    neg_words = 0\n",
        "\n",
        "    for word in word_l:\n",
        "        if (word, 1.0) in freqs:\n",
        "            pos_words += freqs[(word, 1.0)]\n",
        "\n",
        "        if (word, 0.0) in freqs:\n",
        "            neg_words += freqs[(word, 0.0)]\n",
        "\n",
        "    # normalizing to improve accuracy, long tweets filled with common words matter less\n",
        "    total = pos_words + neg_words\n",
        "    if total > 0: #divide by zero error\n",
        "        x[0, 1] = pos_words / total\n",
        "        x[0, 2] = neg_words / total\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "KosqgShlzIbD"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Training Your Model (6 points)\n",
        "\n",
        "To train the model:\n",
        "* Stack the features for all training examples into a matrix `X`.\n",
        "* Call `gradientDescent`, which you've implemented above.\n",
        "* Print the cost J and final weights theta."
      ],
      "metadata": {
        "id": "ThQX8MJUzYQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "m = len(train_x)\n",
        "\n",
        "X_train = np.ones((m, 3))\n",
        "\n",
        "for i in range(m):\n",
        "    X_train[i, :] = extract_features(train_x[i], freqs)\n",
        "\n",
        "Y_train = train_y\n",
        "\n",
        "theta = np.zeros((3, 1))\n",
        "\n",
        "alpha = 0.1\n",
        "num_iters = 1500\n",
        "\n",
        "J_cost, theta_final = gradientDescent(X_train, Y_train, theta, alpha, num_iters)\n",
        "theta = theta_final\n",
        "\n",
        "print(f\"final cost J: {J_cost[-1]}\")\n",
        "print(f\"final theta: \\n{theta_final}\")\n"
      ],
      "metadata": {
        "id": "Bh-KGwH1zXkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c53503-de5e-4d46-fe13-cf734ce82a63"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final cost J: 0.046142775608981425\n",
            "final theta: \n",
            "[[ 0.20682103]\n",
            " [ 4.67293238]\n",
            " [-4.5526649 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E. Test your model (6 points)\n",
        "\n",
        "It is time for you to test your logistic regression function on some new input that your model has not seen before.\n",
        "\n",
        "#### Instructions: Write `predict_tweet`\n",
        "Predict whether a tweet is positive or negative.\n",
        "\n",
        "* Given a tweet, process it, then extract the features.\n",
        "* Apply the model's learned weights on the features to get the logits.\n",
        "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
        "\n",
        "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
      ],
      "metadata": {
        "id": "dZ3BTrKTziUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    # write your code here\n",
        "\n",
        "    x = extract_features(tweet, freqs)\n",
        "\n",
        "    z = np.dot(x, theta)\n",
        "\n",
        "    y_pred = sigmoid(z)\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "C0gsWj6ozgiT"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check performance using the test set\n",
        "After training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n",
        "\n",
        "#### Instructions: Implement `test_logistic_regression`\n",
        "* Given the test data and the weights of your trained model, calculate the accuracy of your logistic regression model.\n",
        "* Use your `predict_tweet()` function to make predictions on each tweet in the test set.\n",
        "* If the prediction is > 0.5, set the model's classification `y_hat` to 1, otherwise set the model's classification `y_hat` to 0.\n",
        "* A prediction is accurate when `y_hat` equals `test_y`.  Sum up all the instances when they are equal and divide by `m`."
      ],
      "metadata": {
        "id": "qfL3YpxMzxEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "\n",
        "   # write your code here\n",
        "\n",
        "    m = len(test_x)\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        tweet = test_x[i]\n",
        "\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        if y_pred > 0.5:\n",
        "          y_hat = 1\n",
        "        else:\n",
        "           y_hat = 0\n",
        "\n",
        "        if y_hat == test_y[i][0]:\n",
        "              correct += 1\n",
        "\n",
        "    accuracy = correct / m\n",
        "    #print(f\"num pos: {np.sum(train_y == 1)}\")\n",
        "    #print(f\"num neg: {np.sum(train_y == 0)}\")\n",
        "    #print(\"final w:\", theta_final)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "3U2Cht1Cz437"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "GwaUhFDlz8wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542c23d6-7b45-4364-e9c0-30ae6ffb352f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.9945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 3. Word Embeddings (30 points)"
      ],
      "metadata": {
        "id": "snPTf5W90sdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this question, you will implement cosine similarities from scratch and solve some word analogy problems with pre-trained word vectors."
      ],
      "metadata": {
        "id": "qujKDYlzgG8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Load Pre-trained GloVe Word Vectors with Gensim (0 points)\n",
        "\n",
        "- Gensim is a package that implements the word2vec family of algorithms. Here, we use it to load a pre-trained word vectors named GloVe ([Pennington et al. 2014](https://aclanthology.org/D14-1162/)). Please follow the instructions below.\n",
        "- See more about the Gensim API here: https://radimrehurek.com/gensim/models/word2vec.html"
      ],
      "metadata": {
        "id": "Y-RqYJcv05rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoQgPZiL0yG1",
        "outputId": "91f7a2d8-a8a9-45d5-851a-1de257f598c8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "k1-WXnpN03RC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKEQ0svQ0_bu",
        "outputId": "a101727e-2220-4072-ee1a-e473637d2275"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('data type:', type(glove_vectors.vectors))\n",
        "print('# words:', glove_vectors.vectors.shape[0])\n",
        "print('Embedding dimension:', glove_vectors.vectors.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emb5Z-y61Bb5",
        "outputId": "5c47afdc-ce31-4d64-975e-3603e4fb64cd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data type: <class 'numpy.ndarray'>\n",
            "# words: 400000\n",
            "Embedding dimension: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Given a query word, find the top 10 words in the vocabulary that have the highest cosine similarity scores (10 points)"
      ],
      "metadata": {
        "id": "ZduEu4W61Lvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vectors.most_similar('cat', topn=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4E-tYXy1Jly",
        "outputId": "077c15bf-2803-4dd3-d215-d4d6149812f7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dog', 0.9218006134033203),\n",
              " ('rabbit', 0.8487821221351624),\n",
              " ('monkey', 0.8041081428527832),\n",
              " ('rat', 0.7891963124275208),\n",
              " ('cats', 0.7865270972251892),\n",
              " ('snake', 0.7798910737037659),\n",
              " ('dogs', 0.7795814871788025),\n",
              " ('pet', 0.7792249917984009),\n",
              " ('mouse', 0.773166835308075),\n",
              " ('bite', 0.7728800177574158)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You need to implement `my_most_similar()` that will do the same thing as the built-in `most_similar()` function above\n",
        "- cosine similarity: $\\frac{v_1 \\cdot v_2}{\\lVert v_1 \\rVert \\lVert v_2 \\rVert}$\n",
        "- $\\lVert v_1 \\rVert$ means the vector norm of $v_1$\n",
        "- We have a vocabulary of 400000 words. You should calculate the cosine similarity between `vec(query_word)` and `vec(any other words)`.\n",
        "- Find the top 10 words that have the highest cosine similarity with the query word.\n",
        "- Return a list of tuple, where each tuple contains a word (str) and its corresponding cosine similarity score (float) to the query word.\n",
        "- The following functions are the only built-in functions you are allowed to use for all the questions in Part 3:\n",
        "  - `glove_vectors.get_index()`, `glove_vectors.get_vector()`, `glove_vectors.index_to_key` (See how to use these functions below.)\n",
        "  - `np.dot()`, `np.sum()`, `np.argsort(), np.transpose()`\n",
        "- Hint: Do **not** include the query word itself in the top 10 words\n",
        "- Hint: To implement $\\sqrt{N}$, you can do `N**0.5`"
      ],
      "metadata": {
        "id": "b2CvdJZ41RcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vectors.get_index('apple') # get the index of a word in the vocabulary"
      ],
      "metadata": {
        "id": "IsWUyrr-nb4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a5aa1e-ee06-4703-d641-ad86d1f8f2a2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3292"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vectors.index_to_key[3292] # index to word"
      ],
      "metadata": {
        "id": "Jg5oUogSneB7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8812a361-d7d9-482c-b2f9-e73a30b48bb2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'apple'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_vectors.get_vector('apple') # get the word vector of a word"
      ],
      "metadata": {
        "id": "_vkbFKQ2nfuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aff93c8-0351-4ad1-8eb1-a900b5125b10"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.52042 , -0.8314  ,  0.49961 ,  1.2893  ,  0.1151  ,  0.057521,\n",
              "       -1.3753  , -0.97313 ,  0.18346 ,  0.47672 , -0.15112 ,  0.35532 ,\n",
              "        0.25912 , -0.77857 ,  0.52181 ,  0.47695 , -1.4251  ,  0.858   ,\n",
              "        0.59821 , -1.0903  ,  0.33574 , -0.60891 ,  0.41742 ,  0.21569 ,\n",
              "       -0.07417 , -0.5822  , -0.4502  ,  0.17253 ,  0.16448 , -0.38413 ,\n",
              "        2.3283  , -0.66682 , -0.58181 ,  0.74389 ,  0.095015, -0.47865 ,\n",
              "       -0.84591 ,  0.38704 ,  0.23693 , -1.5523  ,  0.64802 , -0.16521 ,\n",
              "       -1.4719  , -0.16224 ,  0.79857 ,  0.97391 ,  0.40027 , -0.21912 ,\n",
              "       -0.30938 ,  0.26581 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_most_similar(glove_vectors, query_word, topn):\n",
        "    \"\"\"\n",
        "    Find the most similar words to a given query word based on cosine similarity in the GloVe embedding space.\n",
        "\n",
        "    Args:\n",
        "        glove_vectors (Gensim KeyedVectors)\n",
        "        query_word (str): The word for which to find the most similar words.\n",
        "        topn (int): The number of most similar words to return.\n",
        "\n",
        "    Returns:\n",
        "        list of tuples:\n",
        "            - Each tuple contains a word (str) and its corresponding cosine similarity score (float) to the query word.\n",
        "            - The list is sorted in descending order of cosine similarity.\n",
        "    \"\"\"\n",
        "    # W (numpy.ndarray): glove word embeddings of shape (400000, 50)\n",
        "    W = glove_vectors.vectors\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "    query_vec = glove_vectors.get_vector(query_word)\n",
        "\n",
        "    query_vec_norm = np.sum(query_vec ** 2) ** 0.5\n",
        "\n",
        "    similarities = []\n",
        "\n",
        "    for idx, word in enumerate(glove_vectors.index_to_key):\n",
        "        word_vec = W[idx]\n",
        "\n",
        "        word_vec_norm = np.sum(word_vec ** 2) ** 0.5\n",
        "\n",
        "        numerator = np.dot(query_vec, word_vec)\n",
        "\n",
        "        cos = numerator / (query_vec_norm * word_vec_norm) # (v1*v2) / ||v1||||v2||\n",
        "\n",
        "        if word != query_word:\n",
        "            similarities.append((word, cos))\n",
        "\n",
        "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    my_list = similarities[:topn]\n",
        "\n",
        "    assert len(my_list) == topn\n",
        "    return my_list"
      ],
      "metadata": {
        "id": "z9VKWxnV1PRP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. What are the highest and lowest vector norms among all the word vectors? Answer with two values. (4 points)"
      ],
      "metadata": {
        "id": "9yQZwJ0b1gye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here\n",
        "\n",
        "min_vector_norm = float('inf')\n",
        "max_vector_norm = -float('inf')\n",
        "\n",
        "for word in glove_vectors.index_to_key:\n",
        "    word_vec = glove_vectors.get_vector(word)\n",
        "\n",
        "    norm = np.sum(word_vec ** 2) ** 0.5\n",
        "\n",
        "    if norm < min_vector_norm:\n",
        "        min_vector_norm = norm\n",
        "    if norm > max_vector_norm:\n",
        "        max_vector_norm = norm\n",
        "\n",
        "print(f'max_vector_norm: {max_vector_norm:.3f}, min_vector_norm: {min_vector_norm:.3f}')"
      ],
      "metadata": {
        "id": "IzIc2brD2VdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f974cd92-47db-42a3-bcb2-2433ccb543e0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_vector_norm: 14.122, min_vector_norm: 0.047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Why is cosine similarity better than dot product when calculating similarities between word vectors? (3 points)"
      ],
      "metadata": {
        "id": "PyOJW6dg1rkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The issue with dot product is that it favors long vectors, since it accounts for magnitude the dot product is greater if the vector is longer. More frequent words have longer vectors, and therefore skew their dot product to be greater. Cosine similarility is preferred because it inherently normalizes the calculation, and ignores frequency in it's calculation.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zZU2E2fC2p-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your implementation of `my_most_similar()` should pass the test cases below"
      ],
      "metadata": {
        "id": "PErzUd1816x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diff_results(oracle_list, my_list):\n",
        "  for oracle, mine in zip(oracle_list, my_list):\n",
        "    assert oracle[0] == mine[0], \"find the wrong word\"\n",
        "    assert np.isclose(oracle[1], mine[1]), \"wrong consine similarity\"\n",
        "\n",
        "for query in ['computer', 'frog', 'car']:\n",
        "  oracle_list = glove_vectors.most_similar(query, topn=10)\n",
        "  my_list = my_most_similar(glove_vectors, query, topn=10)\n",
        "  diff_results(oracle_list, my_list)"
      ],
      "metadata": {
        "id": "r4Yi1lWK1c4X"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E. We visualize the word embeddings with PCA below. What do you see in the figure? (3 points)\n",
        "- Hint: Each dot corresponds to a word vector. Do you observe any meaningful direction between related words?\n",
        "- You do not need to write any code in this question"
      ],
      "metadata": {
        "id": "dD41_iZH2EYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "Z = pca.fit_transform(glove_vectors.vectors)"
      ],
      "metadata": {
        "id": "kSAPfG3L19kM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,5))\n",
        "for word in ['king', 'queen', 'lord', 'lady', 'prince', 'princess', 'men', 'women']:\n",
        "  point = Z[glove_vectors.get_index(word)]\n",
        "  plt.scatter(point[0], point[1], color='b')\n",
        "  plt.annotate(word, (point[0], point[1]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "0Jxt-2hc2H8r",
        "outputId": "ff6cab4e-2c16-4914-d6f4-7a3db9bca25e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see here that these word embeddings have captured gender related distinctions, as there is a trend of words with male connotations being at a greater value on the y axis than female connotated words. The X axis seems to effectively distinguish ranks of royalty, with equivalent ranks (queen/king, princess/prince, etc) being nearly in-line on the x-axis. Since the dominance of the words king and queen are different to that of prince or lady, they are spaced apart, and since lord and prince may evoke a similar sense of importance, they are spaced nearby. Words that have similar connotations of importance and gender will therefore be spaced closeby, implying they're related words by 2 metrics.\n",
        "\n"
      ],
      "metadata": {
        "id": "gUyMqZ9v51HY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F. Word Analogy: prince is to princess as lord is to? (10 points)\n",
        "  - word1: `prince`, word2: `princess`, word3: `lord`\n",
        "  - `direction = vec(word2) - vec(word1)`\n",
        "  - `vec_tgt = vec(word3) + direction`\n",
        "  - Calcuate the cosine similarities between `vec_tgt` and all the words in the vocabulary, **except for word3**.\n",
        "  - Return the word that has the highest cosine similarity score."
      ],
      "metadata": {
        "id": "t9crJ6Dx3MCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_analogy(glove_vectors, word1, word2, word3):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        glove_vectors (Gensim KeyedVectors)\n",
        "        word1 (str): The first word in the analogy.\n",
        "        word2 (str): The second word in the analogy.\n",
        "        word3 (str): The third word in the analogy for which to find the analogous word.\n",
        "\n",
        "    Returns:\n",
        "        pred_word (str): The word that best completes the analogy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "    vec1 = glove_vectors.get_vector(word1)\n",
        "    vec2 = glove_vectors.get_vector(word2)\n",
        "    vec3 = glove_vectors.get_vector(word3)\n",
        "    direction = vec2 - vec1\n",
        "    vec_tgt = vec3 + direction\n",
        "\n",
        "    similarities = []\n",
        "\n",
        "    for idx, word in enumerate(glove_vectors.index_to_key):\n",
        "        if word == word3:\n",
        "            continue\n",
        "\n",
        "        word_vec = glove_vectors.get_vector(word)\n",
        "\n",
        "        norm_vectgt = np.sum(vec_tgt ** 2) ** 0.5\n",
        "        norm_wordvec = np.sum(word_vec ** 2) ** 0.5\n",
        "\n",
        "        cos = np.dot(vec_tgt, word_vec) / ((norm_vectgt) * (norm_wordvec))\n",
        "\n",
        "        similarities.append((word, cos))\n",
        "\n",
        "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    pred_word = similarities[0][0]\n",
        "\n",
        "    print(f'{word1} is to {word2} as {word3} is to? {pred_word}')\n",
        "    assert pred_word != word3\n",
        "    return pred_word"
      ],
      "metadata": {
        "id": "i51kuBJr3UW0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_word = word_analogy(glove_vectors, 'prince', 'princess', 'lord')\n",
        "pred_word = word_analogy(glove_vectors, 'aunt', 'uncle', 'queen')\n",
        "pred_word = word_analogy(glove_vectors, 'london', 'england', 'paris')\n",
        "pred_word = word_analogy(glove_vectors, 'cat', 'cats', 'car')"
      ],
      "metadata": {
        "id": "y1e-zPgb9Gul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2564a509-87f7-400e-a7b0-93bdccd7ba3a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prince is to princess as lord is to? lady\n",
            "aunt is to uncle as queen is to? king\n",
            "london is to england as paris is to? france\n",
            "cat is to cats as car is to? cars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRPy1KrRT9Zc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}