{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KQ0G021VWiOh"
      },
      "outputs": [],
      "source": [
        "# Import Libaries\n",
        "import requests\n",
        "import collections\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya2CfD3vWk2_",
        "outputId": "f737aeb2-4d44-4e36-c168-797507920d19"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "response.raise_for_status() # Raise an exception for invalid HTTP status codes\n",
        "text_data = response.text\n",
        "len(text_data), text_data[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtJfs33cWmBJ",
        "outputId": "860cac0e-2c9d-47c8-d1a7-abde2b6a5622"
      },
      "outputs": [],
      "source": [
        "# sample\n",
        "random.seed(42)\n",
        "\n",
        "pos = random.randint(0, len(text_data) - 1000)\n",
        "print(text_data[pos:pos+100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaPIqFwHWpA3",
        "outputId": "ee5c065b-063e-4bca-c0c4-a3daa90aefbe"
      },
      "outputs": [],
      "source": [
        "# preprocessing - do not change\n",
        "def preprocess_text(text_data):\n",
        "  text_data = text_data.replace(',',' , ').replace(';', ' ').replace(':', ' ').replace('.',' . ').replace('?',' ? ').replace('!',' ! ')\n",
        "  text_data = text_data.replace('-', ' ')\n",
        "  text_data = text_data.replace('\\'', '').replace('\"', '')\n",
        "  text_data = text_data.replace('  ', ' ')\n",
        "  text_data = text_data.replace('\\n\\n','\\n').replace('\\n',' </s> <s> ')\n",
        "  text_data = '<s> ' + text_data + ' </s>'\n",
        "  text_data = text_data.lower()\n",
        "  return text_data\n",
        "\n",
        "text_data = preprocess_text(response.text)\n",
        "print(f\"Number of words: {len(text_data.split(' '))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnHQ-enjWqVb",
        "outputId": "cd8182c2-09ae-4a89-dd96-a340d07f5baa"
      },
      "outputs": [],
      "source": [
        "train_data = text_data[:-10_000]\n",
        "test_data = text_data[-10_000:]\n",
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrYnPsMKWrvG",
        "outputId": "cf5169e1-0332-4ee6-8c53-0bdc32be605e"
      },
      "outputs": [],
      "source": [
        "vocab = set(train_data.split(' '))\n",
        "print(f\"Number of unique words: {len(vocab)}\")\n",
        "print(f\"Sample unique words: {list(vocab)[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CzuEDZoZWs7t"
      },
      "outputs": [],
      "source": [
        "def identify_oov_words(corpus, n=3):\n",
        "    \"\"\"\n",
        "    Identify out-of-vocabulary (OOV) words that appear less than `n` times in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: The dataset to process. It should be a dictionary with a 'text' key.\n",
        "    - n: The frequency threshold below which words are considered OOV.\n",
        "\n",
        "    Returns:\n",
        "    - A set of out-of-vocabulary words.\n",
        "    \"\"\"\n",
        "    # INSERT CODE HERE\n",
        "\n",
        "    words = corpus.split(' ')\n",
        "    word_counts = collections.Counter(words)\n",
        "    oov_words = {word for word, count in word_counts.items() if count < n}\n",
        "    return oov_words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3_q0smRW2Ex",
        "outputId": "dbcecb02-905f-4c63-f745-313a0862311e"
      },
      "outputs": [],
      "source": [
        "oov_words = identify_oov_words(train_data)\n",
        "\n",
        "vocab = vocab - oov_words\n",
        "vocab.add('<UNK>')\n",
        "print(f\"Number of OOV words: {len(oov_words)}\")\n",
        "print(f\"Expected number of OOV words: {7181}\")\n",
        "\n",
        "assert len(oov_words) == 7181"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L60G-KIEW5KA"
      },
      "outputs": [],
      "source": [
        "train_data = ' '.join(['<UNK>' if word not in vocab else word for word in train_data.split(' ')])\n",
        "test_data = ' '.join(['<UNK>' if word not in vocab else word for word in test_data.split(' ')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s6p1troiW9lF"
      },
      "outputs": [],
      "source": [
        "uni_counts = collections.defaultdict(lambda:0)\n",
        "bi_counts = collections.defaultdict(lambda:0)\n",
        "tri_counts = collections.defaultdict(lambda:0)\n",
        "four_counts = collections.defaultdict(lambda:0)\n",
        "five_counts = collections.defaultdict(lambda:0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nY3UMFE6W_ys"
      },
      "outputs": [],
      "source": [
        "words = train_data.split(' ')\n",
        "\n",
        "for i in range(len(words)):\n",
        "    uni_counts[words[i]] += 1\n",
        "    if i < len(words) - 1:\n",
        "        bi_counts[(words[i], words[i + 1])] += 1\n",
        "    if i < len(words) - 2:\n",
        "        tri_counts[(words[i], words[i + 1], words[i + 2])] += 1\n",
        "    if i < len(words) - 3:\n",
        "        four_counts[(words[i], words[i + 1], words[i + 2], words[i + 3])] += 1\n",
        "    if i < len(words) - 4:\n",
        "        five_counts[(words[i], words[i + 1], words[i + 2], words[i + 3], words[i + 4])] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-aACGp05XAVW"
      },
      "outputs": [],
      "source": [
        "uni = collections.defaultdict(lambda:0)\n",
        "bi = collections.defaultdict(lambda:0)\n",
        "tri = collections.defaultdict(lambda:0)\n",
        "four = collections.defaultdict(lambda:0)\n",
        "five = collections.defaultdict(lambda:0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CBnpHlliY3wP"
      },
      "outputs": [],
      "source": [
        "total_words = sum(uni_counts.values())\n",
        "\n",
        "for word, count in uni_counts.items():\n",
        "    uni[word] = count / total_words\n",
        "for (w1, w2), count in bi_counts.items():\n",
        "    bi[(w1, w2)] = count / uni_counts[w1]\n",
        "for (w1, w2, w3), count in tri_counts.items():\n",
        "    tri[(w1, w2, w3)] = count / bi_counts[(w1, w2)]\n",
        "\n",
        "for (w1, w2, w3, w4), count in four_counts.items():\n",
        "    four[(w1, w2, w3, w4)] = count / tri_counts[(w1, w2, w3)]\n",
        "\n",
        "for (w1, w2, w3, w4, w5), count in five_counts.items():\n",
        "    five[(w1, w2, w3, w4, w5)] = count / four_counts[(w1, w2, w3, w4)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CUgTaWzVXDsQ"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "assert five[('<s>', 'against', 'the', 'roman', 'state')] == 1.0 # prob of last given prev 4\n",
        "assert four[('remain', '</s>', '<s>', 'i')] == 0.25 # prob of last given prev 3\n",
        "assert tri[('did', 'see', 'and')] == 0.5 # prob of last given prev 2\n",
        "assert bi[('rash', 'like')] == 0.1 # prob of last given prev 1\n",
        "assert round(uni[('citizen')],5) == 0.00031 # prob of last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sVXNg0bZXLfM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_bigram_probability_with_smoothing(word1, word2):\n",
        "\n",
        "    bigram_count = bi_counts[(word1, word2)] + 1\n",
        "    unigram_count = uni_counts[word1] + len(vocab)\n",
        "    return bigram_count / unigram_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pqODu8L-XPS9"
      },
      "outputs": [],
      "source": [
        "def compute_perplexity(data):\n",
        "  \"\"\"\n",
        "    Computes the perplexity of a given text data using a bigram language model.\n",
        "\n",
        "    Parameters:\n",
        "    - data : str\n",
        "    Returns:\n",
        "    - float\n",
        "  \"\"\"\n",
        "\n",
        "  assert len(data.split(' ')) >= 5\n",
        "  # Hint: You should use the math library for exp and log\n",
        "  # INSERT CODE HERE\n",
        "\n",
        "  words = data.split(' ')\n",
        "\n",
        "  log_sum = 0\n",
        "\n",
        "  for i in range(len(words) - 1):\n",
        "      word1 = words[i]\n",
        "      word2 = words[i + 1]\n",
        "      prob = calculate_bigram_probability_with_smoothing(word1, word2)\n",
        "      log_sum += math.log(prob)\n",
        "\n",
        "  perplexity = math.exp(-log_sum / (len(words)))\n",
        "\n",
        "  print(f\"perplexity: {perplexity}\")\n",
        "\n",
        "  return perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVLFr2EfXWOH",
        "outputId": "e688ccfe-a2b7-4555-bc67-ec3bf445699d"
      },
      "outputs": [],
      "source": [
        "assert round(compute_perplexity(test_data)) == 129"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Tw63O_jqwfJ1"
      },
      "outputs": [],
      "source": [
        "# run this cell to import nltk\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HotoYOViw3Bv",
        "outputId": "884cdb22-35d4-4b8f-f92f-7922a95e6f3c"
      },
      "outputs": [],
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "# you only need to run it once\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "O8KazUgww7rU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "lq8Rr5R4xc8d"
      },
      "outputs": [],
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "SJsYMwnoxodO"
      },
      "outputs": [],
      "source": [
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "CVEhgGkix0UR"
      },
      "outputs": [],
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Cp6zvJ3uw-rP"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet):\n",
        "\n",
        "  stemmer = PorterStemmer()\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  processed_tweet = []\n",
        "  for word in tweet_tokens:\n",
        "      if (word not in stop_words and\n",
        "          word not in string.punctuation):\n",
        "          stemmed_word = stemmer.stem(word)\n",
        "          processed_tweet.append(stemmed_word)\n",
        "\n",
        "  return processed_tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "L-s8dg8NxX4i"
      },
      "outputs": [],
      "source": [
        "def build_freqs(tweets, ys):\n",
        "\n",
        "    freqs = {}\n",
        "\n",
        "    for tweet, label in zip(tweets, ys):\n",
        "        processed_tweet = process_tweet(tweet)\n",
        "\n",
        "        label = label.item()\n",
        "\n",
        "        for word in processed_tweet:\n",
        "            pair = (word, label)\n",
        "\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "    return freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "WP5U6cRDyFqb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sigmoid(z):\n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "\n",
        "    # write your code here\n",
        "    z = np.clip(z, -100, 100) #divide by zero error\n",
        "    h = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "u5ylWyKIzEqs"
      },
      "outputs": [],
      "source": [
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "    '''\n",
        "    ### Write your code here\n",
        "\n",
        "    m = len(y)\n",
        "\n",
        "    J_cost = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        z = np.dot(x, theta)\n",
        "        h = sigmoid(z)\n",
        "        h = np.clip(h, 1e-10, (1 - 1e-10))\n",
        "\n",
        "        J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
        "        J_cost.append(J[0][0])\n",
        "\n",
        "        #grad = (alpha/m) *\n",
        "\n",
        "        theta = theta - ((alpha/m) * np.dot(x.T, (h - y)))\n",
        "\n",
        "        #  print(f\"cost: {J[0][0]}\")\n",
        "\n",
        "    return J_cost, theta\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "KosqgShlzIbD"
      },
      "outputs": [],
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3))\n",
        "\n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1\n",
        "\n",
        "    # write your code here\n",
        "    pos_words = 0\n",
        "    neg_words = 0\n",
        "\n",
        "    for word in word_l:\n",
        "        if (word, 1.0) in freqs:\n",
        "            pos_words += freqs[(word, 1.0)]\n",
        "\n",
        "        if (word, 0.0) in freqs:\n",
        "            neg_words += freqs[(word, 0.0)]\n",
        "\n",
        "    # normalizing to improve accuracy, long tweets filled with common words matter less\n",
        "    total = pos_words + neg_words\n",
        "    if total > 0: #divide by zero error\n",
        "        x[0, 1] = pos_words / total\n",
        "        x[0, 2] = neg_words / total\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh-KGwH1zXkF",
        "outputId": "08c53503-de5e-4d46-fe13-cf734ce82a63"
      },
      "outputs": [],
      "source": [
        "freqs = build_freqs(train_x, train_y)\n",
        "\n",
        "m = len(train_x)\n",
        "\n",
        "X_train = np.ones((m, 3))\n",
        "\n",
        "for i in range(m):\n",
        "    X_train[i, :] = extract_features(train_x[i], freqs)\n",
        "\n",
        "Y_train = train_y\n",
        "\n",
        "theta = np.zeros((3, 1))\n",
        "\n",
        "alpha = 0.1\n",
        "num_iters = 1500\n",
        "\n",
        "J_cost, theta_final = gradientDescent(X_train, Y_train, theta, alpha, num_iters)\n",
        "theta = theta_final\n",
        "\n",
        "print(f\"final cost J: {J_cost[-1]}\")\n",
        "print(f\"final theta: \\n{theta_final}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "C0gsWj6ozgiT"
      },
      "outputs": [],
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    # write your code here\n",
        "\n",
        "    x = extract_features(tweet, freqs)\n",
        "\n",
        "    z = np.dot(x, theta)\n",
        "\n",
        "    y_pred = sigmoid(z)\n",
        "\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "3U2Cht1Cz437"
      },
      "outputs": [],
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "\n",
        "   # write your code here\n",
        "\n",
        "    m = len(test_x)\n",
        "\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        tweet = test_x[i]\n",
        "\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        if y_pred > 0.5:\n",
        "          y_hat = 1\n",
        "        else:\n",
        "           y_hat = 0\n",
        "\n",
        "        if y_hat == test_y[i][0]:\n",
        "              correct += 1\n",
        "\n",
        "    accuracy = correct / m\n",
        "    #print(f\"num pos: {np.sum(train_y == 1)}\")\n",
        "    #print(f\"num neg: {np.sum(train_y == 0)}\")\n",
        "    #print(\"final w:\", theta_final)\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwaUhFDlz8wg",
        "outputId": "542c23d6-7b45-4364-e9c0-30ae6ffb352f"
      },
      "outputs": [],
      "source": [
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoQgPZiL0yG1",
        "outputId": "91f7a2d8-a8a9-45d5-851a-1de257f598c8"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "k1-WXnpN03RC"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKEQ0svQ0_bu",
        "outputId": "a101727e-2220-4072-ee1a-e473637d2275"
      },
      "outputs": [],
      "source": [
        "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emb5Z-y61Bb5",
        "outputId": "5c47afdc-ce31-4d64-975e-3603e4fb64cd"
      },
      "outputs": [],
      "source": [
        "print('data type:', type(glove_vectors.vectors))\n",
        "print('# words:', glove_vectors.vectors.shape[0])\n",
        "print('Embedding dimension:', glove_vectors.vectors.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4E-tYXy1Jly",
        "outputId": "077c15bf-2803-4dd3-d215-d4d6149812f7"
      },
      "outputs": [],
      "source": [
        "glove_vectors.most_similar('cat', topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsWUyrr-nb4l",
        "outputId": "c3a5aa1e-ee06-4703-d641-ad86d1f8f2a2"
      },
      "outputs": [],
      "source": [
        "glove_vectors.get_index('apple') # get the index of a word in the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Jg5oUogSneB7",
        "outputId": "8812a361-d7d9-482c-b2f9-e73a30b48bb2"
      },
      "outputs": [],
      "source": [
        "glove_vectors.index_to_key[3292] # index to word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vkbFKQ2nfuN",
        "outputId": "0aff93c8-0351-4ad1-8eb1-a900b5125b10"
      },
      "outputs": [],
      "source": [
        "glove_vectors.get_vector('apple') # get the word vector of a word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "z9VKWxnV1PRP"
      },
      "outputs": [],
      "source": [
        "def my_most_similar(glove_vectors, query_word, topn):\n",
        "    \"\"\"\n",
        "    Find the most similar words to a given query word based on cosine similarity in the GloVe embedding space.\n",
        "\n",
        "    Args:\n",
        "        glove_vectors (Gensim KeyedVectors)\n",
        "        query_word (str): The word for which to find the most similar words.\n",
        "        topn (int): The number of most similar words to return.\n",
        "\n",
        "    Returns:\n",
        "        list of tuples:\n",
        "            - Each tuple contains a word (str) and its corresponding cosine similarity score (float) to the query word.\n",
        "            - The list is sorted in descending order of cosine similarity.\n",
        "    \"\"\"\n",
        "    # W (numpy.ndarray): glove word embeddings of shape (400000, 50)\n",
        "    W = glove_vectors.vectors\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "    query_vec = glove_vectors.get_vector(query_word)\n",
        "\n",
        "    query_vec_norm = np.sum(query_vec ** 2) ** 0.5\n",
        "\n",
        "    similarities = []\n",
        "\n",
        "    for idx, word in enumerate(glove_vectors.index_to_key):\n",
        "        word_vec = W[idx]\n",
        "\n",
        "        word_vec_norm = np.sum(word_vec ** 2) ** 0.5\n",
        "\n",
        "        numerator = np.dot(query_vec, word_vec)\n",
        "\n",
        "        cos = numerator / (query_vec_norm * word_vec_norm) # (v1*v2) / ||v1||||v2||\n",
        "\n",
        "        if word != query_word:\n",
        "            similarities.append((word, cos))\n",
        "\n",
        "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    my_list = similarities[:topn]\n",
        "\n",
        "    assert len(my_list) == topn\n",
        "    return my_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzIc2brD2VdI",
        "outputId": "f974cd92-47db-42a3-bcb2-2433ccb543e0"
      },
      "outputs": [],
      "source": [
        "#Your code here\n",
        "\n",
        "min_vector_norm = float('inf')\n",
        "max_vector_norm = -float('inf')\n",
        "\n",
        "for word in glove_vectors.index_to_key:\n",
        "    word_vec = glove_vectors.get_vector(word)\n",
        "\n",
        "    norm = np.sum(word_vec ** 2) ** 0.5\n",
        "\n",
        "    if norm < min_vector_norm:\n",
        "        min_vector_norm = norm\n",
        "    if norm > max_vector_norm:\n",
        "        max_vector_norm = norm\n",
        "\n",
        "print(f'max_vector_norm: {max_vector_norm:.3f}, min_vector_norm: {min_vector_norm:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "r4Yi1lWK1c4X"
      },
      "outputs": [],
      "source": [
        "def diff_results(oracle_list, my_list):\n",
        "  for oracle, mine in zip(oracle_list, my_list):\n",
        "    assert oracle[0] == mine[0], \"find the wrong word\"\n",
        "    assert np.isclose(oracle[1], mine[1]), \"wrong consine similarity\"\n",
        "\n",
        "for query in ['computer', 'frog', 'car']:\n",
        "  oracle_list = glove_vectors.most_similar(query, topn=10)\n",
        "  my_list = my_most_similar(glove_vectors, query, topn=10)\n",
        "  diff_results(oracle_list, my_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "kSAPfG3L19kM"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "Z = pca.fit_transform(glove_vectors.vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "0Jxt-2hc2H8r",
        "outputId": "ff6cab4e-2c16-4914-d6f4-7a3db9bca25e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,5))\n",
        "for word in ['king', 'queen', 'lord', 'lady', 'prince', 'princess', 'men', 'women']:\n",
        "  point = Z[glove_vectors.get_index(word)]\n",
        "  plt.scatter(point[0], point[1], color='b')\n",
        "  plt.annotate(word, (point[0], point[1]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "i51kuBJr3UW0"
      },
      "outputs": [],
      "source": [
        "def word_analogy(glove_vectors, word1, word2, word3):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        glove_vectors (Gensim KeyedVectors)\n",
        "        word1 (str): The first word in the analogy.\n",
        "        word2 (str): The second word in the analogy.\n",
        "        word3 (str): The third word in the analogy for which to find the analogous word.\n",
        "\n",
        "    Returns:\n",
        "        pred_word (str): The word that best completes the analogy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Your code here\n",
        "\n",
        "    vec1 = glove_vectors.get_vector(word1)\n",
        "    vec2 = glove_vectors.get_vector(word2)\n",
        "    vec3 = glove_vectors.get_vector(word3)\n",
        "    direction = vec2 - vec1\n",
        "    vec_tgt = vec3 + direction\n",
        "\n",
        "    similarities = []\n",
        "\n",
        "    for idx, word in enumerate(glove_vectors.index_to_key):\n",
        "        if word == word3:\n",
        "            continue\n",
        "\n",
        "        word_vec = glove_vectors.get_vector(word)\n",
        "\n",
        "        norm_vectgt = np.sum(vec_tgt ** 2) ** 0.5\n",
        "        norm_wordvec = np.sum(word_vec ** 2) ** 0.5\n",
        "\n",
        "        cos = np.dot(vec_tgt, word_vec) / ((norm_vectgt) * (norm_wordvec))\n",
        "\n",
        "        similarities.append((word, cos))\n",
        "\n",
        "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    pred_word = similarities[0][0]\n",
        "\n",
        "    print(f'{word1} is to {word2} as {word3} is to? {pred_word}')\n",
        "    assert pred_word != word3\n",
        "    return pred_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1e-zPgb9Gul",
        "outputId": "2564a509-87f7-400e-a7b0-93bdccd7ba3a"
      },
      "outputs": [],
      "source": [
        "pred_word = word_analogy(glove_vectors, 'prince', 'princess', 'lord')\n",
        "pred_word = word_analogy(glove_vectors, 'aunt', 'uncle', 'queen')\n",
        "pred_word = word_analogy(glove_vectors, 'london', 'england', 'paris')\n",
        "pred_word = word_analogy(glove_vectors, 'cat', 'cats', 'car')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
